{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d2a0979-c1ef-4cac-813f-ae0423fd5b72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SIADS 593 - Milestone 1\n",
    "\n",
    "*By: Aaron Dankert and Luis Gallego*\n",
    "\n",
    "*Winter 2024*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b8a95-d644-4c22-86a6-c90685429766",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f1afef",
   "metadata": {},
   "source": [
    "#### Motivation and Background\n",
    "Existing research rarely directly addresses aspiring data scientists seeking to strategically launch a career in this field and much of the relevant data and analysis is behind paywalls.$^{1}$ Compounding the issue, the labor market for data science  related talent is undergoing perpetual turmoil – from rapidly evolving toolsets, the influx of talent from new education programs, and the digitization of economies.$^{2}$ The ‘Data Scientist’ occupation itself projected to see among largest changes in labor demand within the next decade.$^{3}$ We are using data from the largest industry surveys to shed light on broad labor market trends for data science talent that may prove actionable for navigating the momentous decisions in their immediate futures. \n",
    "One of the deficiencies of the available data is that it doesn’t use a common measure of compensation, challenging efforts to make inter-country comparisons. The standardization of the data for comparison offers the opportunity to do so by using Purchasing Power Parity (PPP). PPP allows for the comparison of economic indicators across countries in a way that reflects differences in price levels. By using PPP, salaries and other compensation figures can be adjusted to account for the cost of living in each country, enabling more accurate international comparisons. By presenting compensation data in PPP-adjusted terms, we can offer a clearer, more equitable view of global salary trends.\n",
    "Thus an opportunity presented itself to use PPP adjustment to create more meaningful data from the industry surveys for further analysis\n",
    "\n",
    "#### Objectives\n",
    "Presenting PPP-adjusted data, metrics and analysis may offer aspiring data scientists clarity to make informed decisions about where to pursue opportunities, how to evaluate job offers, and what regions might offer the most promising economic benefits for their career growth. Furthermore, how might other features of the survey data interact with PPP adjusted compensation?\n",
    "To reach this question and answer would be complex and time consuming. Given our data we discussed next we will attempt to answer further questions that can shine light on the overarching question:\n",
    "- What do the distributions of observed salaries look like, and is there any skew?\n",
    "- What insights can be provided on change in salaries per country and can we explain this?\n",
    "- Do years of experience make a difference in salary while looking at the education levels of professionals?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01692835-2674-45a1-8531-284d3ff48c2e",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "Both SO and AI salary data sets are public, thus we have voluntary response bias. Most data professionals likely have not completed these surveys, so we are assuming and small sample. There may also be falsifications or data entry errors, there is no way of knowing either, so they are treated like outliers; there were some salary figures up to 7 figures USD. The SO data does not accurately reflect some countries on account of it being “inaccessible to prospective respondents in Crimea, Cuba, Iran, North Korea, and Syria, due to the traffic being blocked by our third-party survey software”.$^{6}$ The AI jobs data has a shorter list of features that lack demographic information compared to SO, and it was not included in the education analysis above. Lastly, the PPP and OECD data does not exist for all countries for all years, even in the time frame chosen. There is likely some meaningful bias on this account where these countries are susceptible to being excluded from data jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6cedec-51df-4b7c-94ef-24d3b280b60b",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "The python environment has been recorded in requirements.txt file. Use pip to install the packages enumerated in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ffb1d13-b001-45cb-936e-27bb00af42b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -r requirements.txt --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db1f154-959c-4294-a7d2-89f5b760100c",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "337f420a-82f8-4f98-ae12-a4038bd85003",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _path: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Third party imports\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maltair\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01malt\u001b[39;00m  \u001b[38;5;66;03m# For declarative statistical visualization\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m  \u001b[38;5;66;03m# For creating static, animated, and interactive visualizations\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m  \u001b[38;5;66;03m# For numerical operations\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m  \u001b[38;5;66;03m# For data manipulation and analysis\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\milestone_1\\Lib\\site-packages\\matplotlib\\__init__.py:142\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_sequence\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\milestone_1\\Lib\\site-packages\\matplotlib\\rcsetup.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fontconfig_pattern\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\milestone_1\\Lib\\site-packages\\matplotlib\\colors.py:57\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _cm, cbook, scale\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_color_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BASE_COLORS, TABLEAU_COLORS, CSS4_COLORS, XKCD_COLORS\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_ColorMapping\u001b[39;00m(\u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, mapping):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\milestone_1\\Lib\\site-packages\\matplotlib\\scale.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _docstring\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mticker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     NullFormatter, ScalarFormatter, LogFormatterSciNotation, LogitFormatter,\n\u001b[0;32m     24\u001b[0m     NullLocator, LogLocator, AutoLocator, AutoMinorLocator,\n\u001b[0;32m     25\u001b[0m     SymmetricalLogLocator, AsinhLocator, LogitLocator)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transform, IdentityTransform\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mScaleBase\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\milestone_1\\Lib\\site-packages\\matplotlib\\ticker.py:138\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms \u001b[38;5;28;01mas\u001b[39;00m mtransforms\n\u001b[0;32m    140\u001b[0m _log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    142\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTickHelper\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFixedFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    143\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNullFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFuncFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatStrFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    144\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStrMethodFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScalarFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultipleLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaxNLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoMinorLocator\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    151\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymmetricalLogLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAsinhLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogitLocator\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\milestone_1\\Lib\\site-packages\\matplotlib\\transforms.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inv\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_path\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     50\u001b[0m     affine_transform, count_bboxes_overlapping_bbox, update_path_extents)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m     53\u001b[0m DEBUG \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _path: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Third party imports\n",
    "import altair as alt  # For declarative statistical visualization\n",
    "import matplotlib.pyplot as plt  # For creating static, animated, and interactive visualizations\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import pycountry  # For country related utilities\n",
    "import pingouin as pg  # For statistical analysis\n",
    "import plotly.express as px  # For creating interactive plots\n",
    "import scipy  # For scientific computing\n",
    "import seaborn as sns  # For statistical data visualization\n",
    "from scipy.stats import norm  # For normal continuous random variable\n",
    "\n",
    "# Local application imports\n",
    "from func_library import *  # Importing all functions from func_library module\n",
    "\n",
    "warnings.filterwarnings('ignore') # To suppress warnings\n",
    "pd.set_option('display.max_columns', 500) # To display all columns\n",
    "np.random.seed(42) # For reproducibility\n",
    "alt.data_transformers.enable('vegafusion') # For enabling altair data transformer and handling large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e9fb6-155f-47ab-872d-ae5cf2c8edcb",
   "metadata": {},
   "source": [
    "# Data Cleaning and Manipulation\n",
    "\n",
    "Processing and feature generation is done using a collection of functions and constant variables defined in func_library.py This is to keep the functions provide additional context via their organization in that file and to modularize sub routines from this project to facitilitate further exploration or export to other, related projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e9d8e5-bfcc-483b-8542-ed9f00fc1c37",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac27af2-649f-4a3d-a96f-03219bef0649",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stack_overflow, skills_list, employments = StackOverflowData.generate_aggregate_df(only_data_science_devs=True)\n",
    "exchange_rate_df = generate_exchange_rates_df()\n",
    "ppp_df = read_ppp()\n",
    "stack_overflow = StackOverflowData.generate_2023_usd_comp(stack_overflow, exchange_rate_df, ppp_df)\n",
    "\n",
    "ai_salaries_df = AISalariesData.generate_df()\n",
    "ai_salaries_df = AISalariesData.generate_2023_usd_comp(ai_salaries_df, exchange_rate_df, ppp_df)\n",
    "ai_salaries_df = pd.concat([ai_salaries_df, pd.get_dummies(ai_salaries_df[\"job_title\"], dtype='int')], axis=1)\n",
    "ai_salaries_df = ai_salaries_df.drop(\"job_title\", axis=1)\n",
    "\n",
    "ai_salaries_df = ai_salaries_df.dropna(subset=['usd_2023'])\n",
    "stack_overflow = stack_overflow.dropna(subset=['usd_2023'])\n",
    "\n",
    "# copies of the original dataframes for charts that need a smaller set of features\n",
    "stack_overflow_chart_df = stack_overflow.copy()\n",
    "ai_salaries_chart_df = ai_salaries_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5116b9fb-4441-4906-92d6-b2b71a565b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevent outliers for our purposes for this data frame\n",
    "relevant_skills = [skill for skill in skills_list if \"hww\" in skill]\n",
    "stack_overflow[\"total_skills\"] = stack_overflow[relevant_skills].sum(axis=1)\n",
    "q3 = stack_overflow[\"usd_2023\"].quantile(0.75)\n",
    "IQR = stack_overflow[\"usd_2023\"].quantile(0.75) - stack_overflow[\"usd_2023\"].quantile(0.25)\n",
    "stack_overflow = stack_overflow[stack_overflow[\"usd_2023\"] < q3 + (1.5*IQR)]\n",
    "\n",
    "s_o = stack_overflow.drop(skills_list, axis=1)\n",
    "\n",
    "# important otherwise we get lots of categorical variables later on\n",
    "s_o[\"edlevel\"] = s_o[\"edlevel\"].replace({\n",
    "    \"Some College\": \"<=Bachelor's\",\n",
    "    \"Associate's\": \"<=Bachelor's\",\n",
    "    \"Secondary\": \"<=Bachelor's\",\n",
    "    \"Elementary\": \"<=Bachelor's\",\n",
    "    \"Else\": \"<=Bachelor's\",\n",
    "    \"Bachelor's\": \"<=Bachelor's\"\n",
    "})\n",
    "\n",
    "# also for our purposes we need to standardize this to integers\n",
    "s_o[\"yearscodepro\"] = s_o[\"yearscodepro\"].replace({'Less than 1 year': 0, 'More than 50 years': 50})\n",
    "s_o[\"yearscodepro\"] = s_o[\"yearscodepro\"].fillna(0)\n",
    "s_o[\"yearscodepro\"] = s_o[\"yearscodepro\"].astype(int)\n",
    "s_o[\"experience_level\"] = pd.cut(\n",
    "                                x=s_o[\"yearscodepro\"], \n",
    "                                bins=[0, 2, 5, 10, np.inf],\n",
    "                                labels=['Entry', 'Middle', 'Senior', 'Executive']\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd2ffb1-0f81-4290-b93c-dd7ba61c5bca",
   "metadata": {},
   "source": [
    "## Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feaebab-f697-4d3e-8955-3ba8cbd80dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the columns are equal so that we can vertically concatenate\n",
    "temp_so = s_o[[\"year\", \"experience_level\", \"country\", \"usd_2023\"] + list(employments)]\n",
    "temp_ai = ai_salaries_df[[\"work_year\", \"experience_level\", \"employee_residence\", \"usd_2023\"] + list(employments)]\n",
    "temp_ai.columns = temp_so.columns\n",
    "\n",
    "# concatenate\n",
    "merged = pd.concat([temp_so, temp_ai], axis=0, ignore_index=True)\n",
    "\n",
    "# get rid of outliers\n",
    "IQR = merged[\"usd_2023\"].quantile(0.75) - merged[\"usd_2023\"].quantile(0.25)\n",
    "q3 = merged[\"usd_2023\"].quantile(0.75)\n",
    "\n",
    "# curious about number of outliers\n",
    "outliers = merged[merged['usd_2023'] > q3 + (1.5*IQR)]\n",
    "print(f\"Outliers: {len(outliers)} vs. Normal: {len(merged)}\")\n",
    "\n",
    "# now get rid of outliers\n",
    "merged = merged[merged['usd_2023'] < q3 + (1.5*IQR)]\n",
    "\n",
    "# final data manipulation, these were strings and below were abbreviations initially\n",
    "merged.loc[:, \"year\"] = merged.loc[:, \"year\"].astype('int64')\n",
    "merged[\"experience_level\"] = merged[\"experience_level\"].replace({'SE': 'Senior', 'MI': 'Middle', 'EN': 'Entry', 'EX': 'Executive'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9939cf35-3309-4028-88c0-d6318d572cee",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76143d7-74b3-49fe-b81c-23da12bddd56",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Helper Functions and Data maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e399622f-c736-4548-bed7-793a5ade76ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map for ISO 3166-1 alpha-2 to country name\n",
    "alpha2_to_name = {country.alpha_2: country.name for country in pycountry.countries}\n",
    "\n",
    "\n",
    "def generate_stacked_barchart(merged, countries):\n",
    "    \"\"\"\n",
    "    Generate a stacked bar chart for the number of statistically significant differences\n",
    "    among countries.\n",
    "    :param merged: the merged data frame\n",
    "    :param countries: the countries we want to compare\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    groupped = merged.groupby(\"year\")\n",
    "    dfs = []\n",
    "    for year, frame in groupped:\n",
    "        t_tests = np.zeros((len(countries), len(countries)))\n",
    "        frame = frame[frame[\"country\"].isin(countries)]\n",
    "        for i in range(len(countries)):\n",
    "            control = countries[i]\n",
    "            for j in range(i+1, len(countries)):\n",
    "                exp = countries[j]\n",
    "                d1 = frame[frame[\"country\"] == control]\n",
    "                d2 = frame[frame[\"country\"] == exp]\n",
    "                result = pg.ttest(d1.usd_2023, d2.usd_2023).round(3) # this is 2 tailed\n",
    "                t_tests[j, i] = result.loc[\"T-test\", \"p-val\"]\n",
    "        df = pd.DataFrame(t_tests, columns=countries, index=countries)\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Create a dictionary to store the counts of statistically significant differences    \n",
    "    stat_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0)))\n",
    "    year = 2019\n",
    "    for df in dfs:\n",
    "        for start in range(len(df)):\n",
    "            r = start\n",
    "            c = 0\n",
    "            dec_row = False\n",
    "            while c < len(df) and r < len(df):\n",
    "                if r == c: # then we are on the diagonal\n",
    "                    dec_row = True\n",
    "                    r += 1\n",
    "                    continue\n",
    "    \n",
    "                row = df.index[r]\n",
    "                col = df.columns[c]\n",
    "                if df.iloc[r, c] < 0.05:\n",
    "                    val = 1\n",
    "                else:\n",
    "                    val = 0\n",
    "                        \n",
    "                if not dec_row: # we are going across\n",
    "                    stat_counts[str(year)][row][col] = val\n",
    "                    c += 1\n",
    "                else:\n",
    "                    stat_counts[str(year)][col][row] = val\n",
    "                    r += 1\n",
    "        year += 1\n",
    "    result = {}\n",
    "    for year, count_dict in stat_counts.items():\n",
    "        counts = [] \n",
    "        for key, value in count_dict.items():\n",
    "            counts.append((key, sum(value.values())))\n",
    "        result[year] = counts\n",
    "    \n",
    "    # Transform the data to a format that can be used to create a DataFrame\n",
    "    transformed_data = {}\n",
    "    for key, tuples in result.items():\n",
    "        for tup in tuples:\n",
    "            index, value = tup[0], tup[1]\n",
    "            if index not in transformed_data:\n",
    "                transformed_data[index] = {}\n",
    "            transformed_data[index][key] = value\n",
    "    \n",
    "    # Create DataFrame from transformed data\n",
    "    working = pd.DataFrame.from_dict(transformed_data, orient='index')\n",
    "    working[\"total\"] = working[[\"2019\", \"2020\", \"2021\", \"2022\", \"2023\"]].sum(axis=1)\n",
    "    working = working.sort_values(\"total\", ascending=False)\n",
    "    \n",
    "    # Create the stacked bar chart\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    working.index = [alpha2_to_name[country] for country in working.index]\n",
    "    working[[\"2019\", \"2020\", \"2021\", \"2022\", \"2023\"]].plot(kind=\"barh\", stacked=True, ax=ax)\n",
    "    ax.set_title(\"Number of Statistically Significant Differences Among Countries\")\n",
    "    ax.set_xlabel('count')\n",
    "    \n",
    "    # Add labels to the bars\n",
    "    # https://www.pythoncharts.com/matplotlib/stacked-bar-charts-labels/\n",
    "    y_offset = -0.4\n",
    "    for bar in ax.patches:\n",
    "      ax.text(\n",
    "          bar.get_x() + bar.get_width() / 2,\n",
    "          bar.get_height() + bar.get_y() + y_offset,\n",
    "          round(bar.get_width()),\n",
    "          ha='center',\n",
    "          color='k',\n",
    "          size=9\n",
    "      )\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39537b6b-9715-469d-bea5-a06f4978eaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a 'Source' column to each dataframe\n",
    "ai_salaries_chart_df['source'] = 'AI-jobs.net'\n",
    "stack_overflow_chart_df['source'] = 'StackOverflow'\n",
    "ai_salaries_chart_df['country'] = ai_salaries_chart_df['company_location']\n",
    "\n",
    "# change all work_year values to strings; rename to 'year'\n",
    "ai_salaries_chart_df['work_year'] = ai_salaries_chart_df['work_year'].astype(str)\n",
    "ai_salaries_chart_df['year'] = ai_salaries_chart_df['work_year']\n",
    "combined_chart_df = pd.concat([ai_salaries_chart_df[['usd_2023', 'source', 'country', 'year']], stack_overflow_chart_df[['usd_2023', 'source', 'country', 'year']]])\n",
    "\n",
    "# eliminate outliers from the combined dataframe\n",
    "combined_chart_q3 = combined_chart_df['usd_2023'].quantile(0.75)\n",
    "combined_chart_q1 = combined_chart_df['usd_2023'].quantile(0.25)\n",
    "combined_iqr = combined_chart_q3 - combined_chart_q1\n",
    "combined_lower_bound = combined_chart_q1 - (1.5 * combined_iqr)\n",
    "combined_upper_bound = combined_chart_q3 + (1.5 * combined_iqr)\n",
    "combined_chart_df = combined_chart_df[(combined_chart_df['usd_2023'] > combined_lower_bound) & (combined_chart_df['usd_2023'] < combined_upper_bound)]\n",
    "\n",
    "# process the combined chart dataframe\n",
    "countries = get_similar_countries(merged, 20)\n",
    "\n",
    "# now we sort the countries by overall averages for later use\n",
    "sort = merged[merged[\"country\"].isin(countries)].groupby(\"country\")[\"usd_2023\"].mean().sort_values()\n",
    "countries = list(sort.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33523328",
   "metadata": {},
   "source": [
    "## Distribution of standardized salary observations in each salary dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714350d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layered_distribution_chart(chart_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    This function creates a layered distribution chart using Altair to compare the distributions of standardized\n",
    "    compensation data from the StackOverflow and AI-jobs.net datasets.\n",
    "    :param chart_df: The combined chart dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    background_color = COLOR_THEME['background']\n",
    "\n",
    "    bar_colors = {\n",
    "        'AI-jobs.net': COLOR_THEME['secondary'],\n",
    "        'StackOverflow': COLOR_THEME['primary']\n",
    "    }\n",
    "\n",
    "    chart = alt.Chart(chart_df.sort_values(by=['source'], ascending=False)).mark_bar(opacity=1,\n",
    "                                                                                        binSpacing=0.3,\n",
    "                                                                                        cornerRadiusTopLeft=4,\n",
    "                                                                                        cornerRadiusTopRight=4).encode(\n",
    "        alt.X('usd_2023:Q', bin=alt.Bin(maxbins=50), title='USD 2023 Salary'),\n",
    "        alt.Y('count()', stack=None, title='Count'),\n",
    "        alt.Color('source:N', legend=alt.Legend(title=\"Dataset\", orient='top-right', \n",
    "                                                fillColor=background_color, labelColor=COLOR_THEME['text'], \n",
    "                                                titleColor=COLOR_THEME['text'], titlePadding=5, \n",
    "                                                labelLimit=200, padding=5), \n",
    "                scale=alt.Scale(domain=list(bar_colors.keys()), range=list(bar_colors.values()))),\n",
    "    ).properties(\n",
    "        width=600,\n",
    "        height=400,\n",
    "        title=\"Salary Distribution Comparison\",\n",
    "        background=background_color\n",
    "    ).configure_view(\n",
    "        stroke=background_color,\n",
    "        fill=background_color\n",
    "    ).configure_axis(\n",
    "        labelColor=COLOR_THEME['text'],\n",
    "        titleColor=COLOR_THEME['text']\n",
    "    )\n",
    "    return chart\n",
    "\n",
    "dist_chart = layered_distribution_chart(combined_chart_df)\n",
    "dist_chart.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda63668",
   "metadata": {},
   "source": [
    "This chart demonstrates a fundamental issue combining these datasets -- that they do not share the same distribution and therefore could not be representative of the same population. A violation of an assumption that is important to be cognisent of."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85dda25-fe7f-4507-9b3d-282f83239804",
   "metadata": {},
   "source": [
    "## Mean Compensation Estimates for Represented Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8205cafa-ecff-4594-bdee-209c11ab1d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_half = countries[:len(countries)//2]\n",
    "second_half = countries[len(countries)//2:]\n",
    "length = max(len(first_half), len(second_half))\n",
    "\n",
    "fig, axes = plt.subplots(length, 2, sharex=True, figsize=(9, 20))\n",
    "\n",
    "# iterate over the axes and the countries\n",
    "for i in range(length):\n",
    "    working = merged[merged[\"country\"] == first_half[i]]\n",
    "    mean_value = working[\"usd_2023\"].mean()\n",
    "    \n",
    "    param = norm.fit(working[\"usd_2023\"]) # distribution fitting\n",
    "    x = np.linspace(working[\"usd_2023\"].min(), working[\"usd_2023\"].max(), 100)\n",
    "    pdf_fitted = norm.pdf(x, *param)   \n",
    "    \n",
    "    axes[i, 0].plot(x, pdf_fitted, color='black', label=\"Normal distribution\")\n",
    "    axes[i, 0].axvline(mean_value, color='red', linestyle='dashed', linewidth=2, label='Mean')\n",
    "    axes[i, 0].hist(working[\"usd_2023\"], bins=20, alpha=0.5, density=True, label=\"Actual distribution\", color='olive')\n",
    "    axes[i, 0].set_yticks([])\n",
    "    axes[i, 0].set_ylabel(\"\\n\".join(alpha2_to_name[first_half[i]].split(\" \")), fontsize=16)\n",
    "    axes[i, 0].text(200000, 7e-6, f\"n={len(working)}\")\n",
    "\n",
    "    working = merged[merged[\"country\"] == second_half[i]]\n",
    "    mean_value = working[\"usd_2023\"].mean()\n",
    "    \n",
    "    param = norm.fit(working[\"usd_2023\"])\n",
    "    x = np.linspace(working[\"usd_2023\"].min(), working[\"usd_2023\"].max(), 100)\n",
    "    pdf_fitted = norm.pdf(x, *param)   \n",
    "    \n",
    "    axes[i, 1].plot(x, pdf_fitted, color='black', label=\"Normal distribution\")\n",
    "    axes[i, 1].axvline(mean_value, color='red', linestyle='dashed', linewidth=2, label='Mean')\n",
    "    axes[i, 1].hist(working[\"usd_2023\"], bins=20, alpha=0.5, density=True, label=\"Actual distribution\", color='olive')\n",
    "    axes[i, 1].set_yticks([])\n",
    "    axes[i, 1].set_ylabel(\"\\n\".join(alpha2_to_name[second_half[i]].split(\" \")), fontsize=16)\n",
    "    axes[i, 1].text(200000, 7e-6, f\"n={len(working)}\")\n",
    "\n",
    "fig.text(0.5, 0.08, 'Adjusted Salaries in USD', ha='center', fontsize=16)\n",
    "fig.text(0.5, 0.89, 'Observed Salary Values Over Mean and Normal Distribution', ha='center', fontsize=16)\n",
    "\n",
    "plt.subplots_adjust(hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bed658-e4b1-4c98-a4d8-94eabc5f827e",
   "metadata": {},
   "source": [
    "Above we have the actual distributions in olive with the normal overlayed in black; the mean salary from 2019-2023 is in a dashed red line. We can see that the chart reads from top to bottom, and once at the bottom to the right then top to bottom again. It can be seen that very large samples, like in the US, result in an almost normal distribution, while smaller values are skewed right, like in Romania. An exception to this is India where it it heavily skewed right, as opposed to the Great Britain which has a comparable number of responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eb2b0b-8f56-4cce-b693-abe96bafb8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_stacked_barchart(merged, countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa31018-15a7-4f02-89d3-0632f3c4b455",
   "metadata": {},
   "source": [
    "This bar chart, is complicated in the sense that the higher the number the more statistically significant differences there are. T-tests were done on each combination and this bar chart was created after ordering the total number of differences. Year is encoded by color. Unsurprisingly, the US was at the top, likely because of it's high salaires, and so was India and Brazil even more surprisingly. If we take a look at some of these means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da02606d-e9b8-41b9-bd9a-bb27c64f2f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in countries:\n",
    "    print(f\"{c} mean: ${merged[merged['country'] == c]['usd_2023'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e47659-d8b2-452a-8d26-05f8ecece6b6",
   "metadata": {},
   "source": [
    "Now we get a more definitive look as to why the US and Israel have such big differences. While Romania does not despite having a comparable overall mean. Looking at the first figure we see that Romania has a sample of 290 vs the US of more than 20,000. This factors into the equation of the t-test where the standard deviation is divided by the sample size (n). Large values of n mean that deviations from the null hypothesis (that both populations have the same mean) have a statistical effect than smalle samples do, or that's it's less uncertain the smaller the sample size. It's likely that given this fact and Romania's distribution, that when compared to other small sampled countries that the effects described earlier are smaller than than when compared to the US's.\n",
    "\n",
    "We see the same with India as the sample size is much larger than othe countries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506024c2-e899-46ce-b3a7-da756e057bed",
   "metadata": {},
   "source": [
    "## Percent Changes in Countries Per Year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f34a24e-c26c-4c73-a4ef-6c8affb7f1d6",
   "metadata": {},
   "source": [
    "Making a quick color palette below from group agreed upon colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d2e846-bcc1-4533-a3f1-307687f58cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Define the colors\n",
    "positive_blue = \"#1779BA\"\n",
    "negative_yellow = \"#FFCB05\"\n",
    "\n",
    "# Create a custom diverging colormap\n",
    "colors = [(0, negative_yellow), (0.5, 'white'), (1, positive_blue)]\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"Custom Colormap\", colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984b66ea-2752-48c1-9d64-c25312e60d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pivot = pd.pivot_table(merged, values=\"usd_2023\", index=\"country\", columns=\"year\", aggfunc=\"mean\")\n",
    "pivot = pivot.dropna()\n",
    "\n",
    "pivot = pivot.loc[countries, :]\n",
    "pivot = pivot.pct_change(axis=1)\n",
    "pivot.columns = [\"2019\"] + [f\"{int(year)-1}-{year}\" for year in pivot.columns[1:]]\n",
    "pivot = pivot.drop(\"2019\", axis=1)\n",
    "pivot.index = [alpha2_to_name[country] for country in pivot.index]\n",
    "\n",
    "plt.title(\"Percent Change For Average Salary per Country per Year\")\n",
    "ax = sns.heatmap(pivot, annot=True, center=0, fmt=\"0.01%\", cmap=custom_cmap, linecolor='k', linewidths=0.5)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_ticks([-0.4,  -0.2, 0, 0.2, 0.4])\n",
    "cbar.set_ticklabels(['-40%', '-20%', '0%', '20%', '40%'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a8d3f-e9cd-4aed-8838-2350b7a94bda",
   "metadata": {},
   "source": [
    "A deeper dive into the changer per year instead of the overall 2019-2023 gives us more actionable insights. This allows data professionals to see where their skills are most valued and where there is volatility. It can be seen that Argentina and Russia have the most volatility with their salaries. These relatively large increases with salary may not have to do with the skills that data professionals but rather with the volatile economies of these countries.\n",
    "\n",
    "Russia invaded Ukraine in these years and as a result we see the sharp decreases in salary as their economy collapsed. Argentina on the other hand has been under economic distress and saw growth in salaries the last year it saw GDP growth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10601b6c-3cb4-488e-a30a-7632c140d127",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_skills = [skill for skill in skills_list if \"hww\" in skill]\n",
    "stack_overflow[\"total_skills\"] = stack_overflow[relevant_skills].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91788db1-0506-4eee-9e9c-d4e06393461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = pd.pivot_table(stack_overflow, values=\"total_skills\", index=\"country\", columns=\"year\", aggfunc=\"mean\")\n",
    "pivot = pivot.dropna()\n",
    "pivot = pivot.loc[countries, :]\n",
    "pivot = pivot.pct_change(axis=1)\n",
    "pivot.columns = [\"2019\"] + [f\"{int(year)-1}-{year}\" for year in pivot.columns[1:]]\n",
    "pivot = pivot.drop([\"2019\", \"2022-2023\"], axis=1)\n",
    "pivot.index = [alpha2_to_name[country] for country in pivot.index]\n",
    "\n",
    "plt.title(\"Percent Change in the Average Number of Skills per Country\")\n",
    "ax = sns.heatmap(pivot, annot=True, center=0, fmt=\"0.01%\", cmap=custom_cmap, linecolor='k', linewidths=0.5)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_ticks([-0.2, -0.10, 0, 0.1, 0.2, 0.3])\n",
    "cbar.set_ticklabels(['-20%', '-10%', '0%', '10%', '20%', '30%'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe40399-bd4e-4a41-8fd2-69d3237419c9",
   "metadata": {},
   "source": [
    "This heatmap and the ones below did not make it into our slides due to space constraints. But we can still extract insights here. We see that ordering by the overall means still holds here, so we can see how skills change over time. Above we created the `total_skills` column in the `stack_overflow` data frame for use to get this pivot table turned into a heat map. We see a decline in skills in the US, likely because of the growth in low or no code options over the recent years that the StackOverflow survey did not cover. We also see that any relatively large increase in skills comes with its own sharp decrease from that year, notably in Romania and Russia. This could be due to sampling bias or the invasion of Ukraine in Russia's case. Sampling bias is hypothesized because 2023 is not included in this chart. This is because we get sharp decreases across all countries due to the size. Below it can be seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b308e9-f275-4ee7-8870-39310e5c9bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_overflow.groupby('year')['count'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19de29eb-74b8-439b-81af-cb595841e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = pd.pivot_table(merged, values=\"usd_2023\", index=\"experience_level\", columns=\"year\", aggfunc=\"mean\")\n",
    "pivot = pivot.dropna()\n",
    "# pivot = pivot[pivot[\"country\"].isin(countries)]\n",
    "# pivot = pivot.loc[countries, :]\n",
    "pivot = pivot.pct_change(axis=1)\n",
    "pivot.columns = [\"2019\"] + [f\"{int(year)-1}-{year}\" for year in pivot.columns[1:]]\n",
    "pivot = pivot.drop(\"2019\", axis=1)\n",
    "plt.title(\"Average Salary Change Based on Experience\")\n",
    "ax = sns.heatmap(pivot, annot=True, center=0, fmt=\"0.01%\", cmap=custom_cmap, linecolor='k', linewidths=0.5)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_ticks([0.0, 0.05, 0.1, 0.15, 0.20, 0.25, 0.3])\n",
    "cbar.set_ticklabels(['0%', '5%', '10%', '15%', '20%', '25%', '30%'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2729aa7f-7015-4204-8849-f66ac5f586d7",
   "metadata": {},
   "source": [
    "Even further we can granularize the merged salary data into their respective salary change based on experience. The stack overflow data required binning the years of experience, which were integers, into categories, which were named into what is seen here and is as follows:\n",
    "- 0-2 years = entry\n",
    "- 2-5 years = middle\n",
    "- 5-10 years = senior\n",
    "- +10 years = executive\n",
    "\n",
    "Worldwide we see enormous growths in salary across all experience levels., especially among the middle level data professionals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945163cb-d6a3-4a50-8577-5e311b3086aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = pd.pivot_table(merged, values=\"usd_2023\", index=\"experience_level\", columns=\"year\", aggfunc=\"count\")\n",
    "pivot = pivot.dropna()\n",
    "pivot = pivot.pct_change(axis=1)\n",
    "pivot.columns = [\"2019\"] + [f\"{int(year)-1}-{year}\" for year in pivot.columns[1:]]\n",
    "pivot = pivot.drop(\"2019\", axis=1)\n",
    "\n",
    "plt.title(\"Change in Experience Levels Overtime\")\n",
    "sns.heatmap(pivot, annot=True, center=0, fmt=\"0.01%\", cmap=custom_cmap, linecolor='k', linewidths=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8616d28f-422f-49df-bebe-1d8059f81992",
   "metadata": {},
   "source": [
    "Counting the number of experience levels and getting the percent change nets us the complete opposite of what is above. Mostly we see drops in the number of these experience levels. This is counter intuitive since with less people in these categories we see a much larger increase in salaries. We can say that the salaries are increasing for entry level and executive positions, while the number of these positions, or people who hold them, are decreasing. Decreasing due to not wanting the job any more or switching careers is unsure. Stranger so, it's seen that the senior level has a dramatic increase of more than doubling between 2022-2023.\n",
    "\n",
    "This chart does not capture any changes of individuals incrementing their experience level, and would be impossible given we don't have personal data, and it would be unethical for this analysis, since it's not needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a30a987-94f7-422d-81ff-f8e1718776c0",
   "metadata": {},
   "source": [
    "## Levels of Degree and Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507bb08f-1b98-4cc8-b527-4fffd68a0710",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "    \"analyst\": \"#FFCB05\", \n",
    "    \"data_scientist\": \"#00274C\", \n",
    "    \"developer\": \"#131516\", \n",
    "    \"engineer_other\": \"#D86018\", \n",
    "    \"management\": \"#2A5993\", \n",
    "    \"scientist_other\": \"#A5A508\", \n",
    "    \"systems_architect\": \"#663289\"\n",
    "}\n",
    "\n",
    "for job in employments:\n",
    "    temp = s_o[(s_o[job] == 1) & (s_o[\"yearscodepro\"] < 15)]\n",
    "    temp1 = temp.sample(50)\n",
    "    x = temp1[\"yearscodepro\"]\n",
    "    y = temp1[\"usd_2023\"]\n",
    "    plt.plot(x, y, 'o', label=job, alpha=0.5, color=colors[job])\n",
    "    for j in range(1, len(knots)):\n",
    "        gt = knots[j-1]\n",
    "        lt = knots[j]\n",
    "        grouped = temp[temp[\"yearscodepro\"].between(gt, lt)][\"usd_2023\"].mean()\n",
    "        plt.hlines(grouped, xmin=lt, xmax=gt, color=colors[job], zorder=5, linewidth=2)\n",
    "\n",
    "plt.title(\"Years Of Experience Professional Coding vs. Salary and Job Type\")\n",
    "plt.xlabel(\"Years of Experience Professionally Coding\")\n",
    "plt.ylabel(\"Adjusted Salary in USD\")\n",
    "plt.legend(loc=(1, 0.55))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79cf4f8-a863-4b73-ba83-a7ecb1b98207",
   "metadata": {},
   "source": [
    "Though a noisy chart, this is the sampled version from the code snippet above. Otherwise, this would be incredibly noisy. The mark bars are the mean for this education level, where anythin less than a bachelor's is grouped together, which is also why it's the largest group. Doctoral degrees dominate the salary to no surprise due to the rigor of this degree. The education salary data is as expected following doctoral degrees, then masters, then bachelor's or less. Surprisingly, professional education last. Professional education is surprisingly in this data, which can include doctors and those who completed law school. These respondents have the lowest salary, consistently. Likely due to their education not being in a computer focused curriculum, and they switched to some sort of data profession. Or they may happen to code in their free time and use StackOverflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a67e871-f558-4693-b274-b13715f5098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "    \"analyst\": \"#FFCB05\", \n",
    "    \"data_scientist\": \"#00274C\", \n",
    "    \"developer\": \"#131516\", \n",
    "    \"engineer_other\": \"#D86018\", \n",
    "    \"management\": \"#2A5993\", \n",
    "    \"scientist_other\": \"#A5A508\", \n",
    "    \"systems_architect\": \"#663289\"\n",
    "}\n",
    "\n",
    "knots = [0, 3, 6, 9, 12, 15]\n",
    "\n",
    "for job in employments:\n",
    "    temp = s_o[(s_o[job] == 1) & (s_o[\"yearscodepro\"] < 15)]\n",
    "    temp1 = temp.sample(50)\n",
    "    x = temp1[\"yearscodepro\"]\n",
    "    y = temp1[\"usd_2023\"]\n",
    "    plt.plot(x, y, 'o', label=job, alpha=0.5, color=colors[job])\n",
    "    for j in range(1, len(knots)):\n",
    "        gt = knots[j-1]\n",
    "        lt = knots[j]\n",
    "        grouped = temp[temp[\"yearscodepro\"].between(gt, lt)][\"usd_2023\"].mean()\n",
    "        plt.hlines(grouped, xmin=lt, xmax=gt, color=colors[job], zorder=5, linewidth=2)\n",
    "\n",
    "plt.title(\"Years Of Experience Professional Coding vs. Salary and Job Type\")\n",
    "plt.xlabel(\"Years of Experience Professionally Coding\")\n",
    "plt.ylabel(\"Adjusted Salary in USD\")\n",
    "plt.legend(loc=(1, 0.55))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679b19d7-325d-4565-8e90-95cebcbe06c4",
   "metadata": {},
   "source": [
    "Machine learning engineers and other kinds of engineers were grouped into the engineer_other category. We see that early in the career that engineers dominate, then as time goes on we see data scientists dominating the salaries with engineers. This is likely to the boom in data scientists resulting in more people with experience to mentor those within the workplace. Analysts are lower on the average salary for our time ranges, as well as system architects. This is strange considering how closely an analysts job is with a data scientists, as well as the need for cloud and system architects in a growing digital world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af25526",
   "metadata": {},
   "source": [
    "## Choropleth of % Changes in Standardized Compensation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d8259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choropleth_of_deltas(choro_chart_df: pd.DataFrame) -> alt.Chart:\n",
    "    df_2019 = choro_chart_df[choro_chart_df['year'] == '2019'] \n",
    "    df_2023 = choro_chart_df[choro_chart_df['year'] == '2023']\n",
    "\n",
    "    # Only keep countries with 10+ observations\n",
    "    country_counts_2019 = df_2019.country.value_counts()\n",
    "    country_counts_2023 = df_2023.country.value_counts()\n",
    "    valid_countries = country_counts_2019[country_counts_2019 >= 10].index.intersection(\n",
    "                        country_counts_2023[country_counts_2023 >= 10].index)\n",
    "\n",
    "    df_2019 = df_2019[df_2019['country'].isin(valid_countries)]\n",
    "    df_2023 = df_2023[df_2023['country'].isin(valid_countries)]\n",
    "\n",
    "    # Calculate averages for each country and year\n",
    "    df_2019 = df_2019.groupby('country')['usd_2023'].mean().reset_index()\n",
    "    df_2023 = df_2023.groupby('country')['usd_2023'].mean().reset_index()\n",
    "\n",
    "    # Merge and calculate deltas\n",
    "    change_19_23_df = df_2019.merge(df_2023, on='country')\n",
    "    change_19_23_df['delta'] = change_19_23_df['usd_2023_y'] - change_19_23_df['usd_2023_x']\n",
    "    change_19_23_df['pct_change'] = change_19_23_df['delta'] / change_19_23_df['usd_2023_x'] * 100\n",
    "    change_19_23_df = change_19_23_df[['country', 'usd_2023_x', 'usd_2023_y', 'delta', 'pct_change']]\n",
    "    change_19_23_df.columns = ['country', '2019_comp', '2023_comp', 'delt', 'percent_change']\n",
    "\n",
    "    #from vega_datasets import data\n",
    "    change_19_23_df['country_alpha_3'] = change_19_23_df['country'].apply(lambda x: convert_country_alpha2_to_alpha3(x))\n",
    "\n",
    "    # Create the choropleth map with Plotly Express\n",
    "    fig = px.choropleth(change_19_23_df,\n",
    "                        locations='country_alpha_3',\n",
    "                        color='percent_change',\n",
    "                        hover_name='country_alpha_3',\n",
    "                        color_continuous_scale=[COLOR_THEME['bold'], 'lightgrey', COLOR_THEME['secondary']],\n",
    "                        color_continuous_midpoint=0)\n",
    "\n",
    "    fig.update_geos(\n",
    "        landcolor=COLOR_THEME['alt_background'],  # Neutral color for land\n",
    "        lakecolor=COLOR_THEME['primary'],  # Color for lakes\n",
    "        oceancolor=COLOR_THEME['primary'],  # Color for ocean\n",
    "        showocean=True,\n",
    "        showcountries=True # Show country borders\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        paper_bgcolor=COLOR_THEME['alt_background'],\n",
    "        plot_bgcolor=COLOR_THEME['alt_background'],\n",
    "        title_font_color=COLOR_THEME['text'],\n",
    "        font_color=COLOR_THEME['text'],\n",
    "        geo=dict(\n",
    "            bgcolor=COLOR_THEME['alt_background'],\n",
    "            showframe=False,\n",
    "            showcoastlines=False,\n",
    "            projection_scale=1.1,\n",
    "            center=dict(lat=30, lon=0), # change the center of the map\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, t=0, b=0)  # Minimize margins around the map\n",
    "    )\n",
    "\n",
    "    fig.update_coloraxes(colorbar=dict(\n",
    "        title=r'Net % change in average salary 2019-2023',\n",
    "        titleside='right',\n",
    "        titlefont=dict(size=12, color=COLOR_THEME['text']),\n",
    "        tickfont=dict(color=COLOR_THEME['text']),\n",
    "        bgcolor=COLOR_THEME['alt_background'],\n",
    "        len= 0.8,\n",
    "        y=.42\n",
    "    ))\n",
    "    return fig\n",
    "\n",
    "choro_chart = choropleth_of_deltas(combined_chart_df)\n",
    "choro_chart.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6882dad",
   "metadata": {},
   "source": [
    "This figure illustrates the percentage change in compensation for data professionals from 2019 to 2023. Countries are color-coded to reflect increases or decreases in salary, offering a clear, visual representation of global compensation trends. The gradations of color from light to dark indicate the range of percentage changes, with the scale provided for precise interpretation. The countries not encoded with colors from the scale were not included in the visualization because they did not have sufficient number of observations to be considered representative if they had any at all.\n",
    "Here we observe that compensation in China, India, and Colombia have been on tear recently. China has seen improvements of ~76%, India has grown it’s data professionals’ mean compensation by 60%, whereas Colombia saw its data professional salaries double in those four years. Recall these movements have been controlled for changes in purchasing power and are genuine increases in genuine increases in the standard of living for these professionals. \n",
    "Among the unhappiest data professional have to be those residing in Russia where compensation has fallen ~36%. Russian workers have been impacted by the sanctions imposed for their incursion into Ukraine.$^{4}$ This is not a viable excuse for other countries’ atrophying compensation packages. Vietnam saw its data professionals lose 42% of their purchasing ability. Norway also saw a plunge in compensation of ~21%.\n",
    "Emerging data professionals can leverage this map to strategically plan the launch of their careers by identifying geographic regions with robust growth in compensation. Considering the upward trends in countries like China, India, and Colombia, these locations may offer promising opportunities for personal and professional development. Conversely, awareness of regions with declining compensation, such as Russia and Vietnam, is crucial for career risk management. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94b05e9-443f-4a54-9968-4e2067095f66",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Coresignal. (2024, February 15). Always Fresh Public Web Data. Retrieved from https://coresignal.com/\n",
    "2. U.S. Bureau of Labor Statistics. (2023, September 6). Data scientists. Occupational Outlook Handbook. Retrieved from https://www.bls.gov/ooh/math/data-scientists.htm\n",
    "3. OECD. (2019). Digitalisation and productivity. Retrieved from https://www.oecd.org/economy/growth/digitalisation-productivity-and-inclusiveness/\n",
    "4. Lambertucci, C. (2023, November 18). The daunting economic landscape choking Argentine voters. El Pais. Retrieved from https://english.elpais.com/international/2023-11-18/the-daunting-economic-landscape-choking-argentine-voters.html\n",
    "5. Lindsay, J. M. (2020, December 17). Ten most significant world events in 2020. Council on Foreign Relations. Retrieved from https://www.cfr.org/blog/ten-most-significant-world-events-2020\n",
    "6. Reuters. (2022, April 14). Russian workers face new reality as Ukraine war sanctions sap job prospects. Retrieved from https://www.reuters.com/business/russian-workers-face-new-reality-ukraine-war-sanctions-sap-job-prospects-2022-04-13/\n",
    "7. Stack Overflow. (2023). StackOverflow Developer Survey 2023. Retrieved from https://survey.stackoverflow.co/2023/#methodology-general\n",
    "\n",
    "\\* Some references only used in accompanying report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e7d8da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b251cc7c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
