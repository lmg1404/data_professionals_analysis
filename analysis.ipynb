{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "460bcd30-1c2e-4756-ba10-bfe0a0747233",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0863b356-bcad-4fb1-8b8a-eb91a6016e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pycountry\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer\n",
    "import pingouin as pg\n",
    "# import warnings\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 500)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a080ab69-f57c-459a-8a01-bafb2ef2e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99d2e08-d067-414d-bf79-44143c6c6191",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "108206fc-d649-4506-8234-c0f970171086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ppp() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads PPP csv and returns resulting data frame\n",
    "    Data Manipulations:\n",
    "    - Only use years since we're not going to use anything before unless we go historic route\n",
    "    - Fill nans with string type Null\n",
    "    - - thought process is to us other functions that will detect str type and throw an error\n",
    "    - - if nan operation will probably go through, so doing isinstance == str would be best probably\n",
    "    - Also index columns are the country code, should match to the PyCountry library\n",
    "    \n",
    "    Inputs: None\n",
    "    Output: pd.DataFrame\n",
    "    \"\"\"\n",
    "    years = [\"2019\", \"2020\", \"2021\", \"2022\"]\n",
    "    ppp = pd.read_csv(\"data/ppp.csv\", header=2, index_col=\"Country Code\")[years]\n",
    "    # ppp = ppp.fillna(\"Null\") # this way we can control the type, so we can create a function that checks type before anything else\n",
    "    return ppp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b67c2165-6e83-49e2-9d9c-a16a05e99987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walrus_helper(salaries: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Helper function that is just a for loop that goes through unique job titles and assigns a basic name\n",
    "\n",
    "    Input: pd.DataFrame\n",
    "    Output: dict{str: str}\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for job in list(salaries[\"job_title\"].unique()):\n",
    "        if (short := \"Analyst\") in job:\n",
    "            mapping[job] = short.lower()\n",
    "    \n",
    "        elif (short := \"Engineer\") in job:\n",
    "            mapping[job] = short.lower() + \"_other\"\n",
    "            \n",
    "        elif (short := \"Data Scientist\") in job or \"Data Science\" in job:\n",
    "                mapping[job] = '_'.join(short.lower().split(\" \"))\n",
    "            \n",
    "        elif \"Architect\" in job:\n",
    "            mapping[job] = \"systems_architect\"\n",
    "    \n",
    "        elif \"Manager\" in job:\n",
    "            mapping[job] = \"management\"\n",
    "    \n",
    "        elif (short := \"Developer\") in job:\n",
    "            mapping[job] = short.lower()\n",
    "            \n",
    "        elif \"math\" in job.lower() or \"stat\" in job.lower():\n",
    "            mapping[job] = \"mathematician_statistician\"\n",
    "            \n",
    "        else:\n",
    "            mapping[job] = \"scientist_other\"\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "575b5e8d-2564-4f3d-9104-dbf8f747fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_salaries() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads the salaries from ai-net and returns them into a dataframe\n",
    "    Data Manipulation:\n",
    "    - Change 2 letter country names into 3 letter names for uniformity\n",
    "    - Map above function in job_title to simpler names\n",
    "    - Only taking 2020 - 2023, we have no data on 2024\n",
    "    \n",
    "    Input: None\n",
    "    Output: pd.DataFrame\n",
    "    \"\"\"\n",
    "    salaries = pd.read_csv(\"data/salaries.csv\")\n",
    "    country_abbreviations = {country.alpha_2: country.alpha_3 for country in pycountry.countries}\n",
    "    mapping = walrus_helper(salaries)\n",
    "    \n",
    "    salaries[[\"employee_residence\", \"company_location\"]] = salaries[[\"employee_residence\", \"company_location\"]].replace(country_abbreviations)\n",
    "    salaries[\"job_title\"] = salaries[\"job_title\"].replace(mapping)\n",
    "    salaries = salaries[salaries[\"work_year\"] < 2024]\n",
    "    \n",
    "    return salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59d37b96-23a6-45d2-a40f-0813123c1395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_onehot_skills(frames: dict) -> None:\n",
    "    \"\"\"\n",
    "    Given a dictionary of pandas dataframes we want to one hot the skills in particular.\n",
    "    We want to take the skills in the different columns and one hot them such we can sum them for groupby operations.\n",
    "    We get a dictionary of pandas DataFrames and perform an inplace operation such that we don't have to create new memory.\n",
    "    Return a dictionary of a list of strings for a couple reasons:\n",
    "        - there's no way we will remember all of these so automation by putting these into a list seemed like the best idea\n",
    "        - the keys will match those in the input in case we want to do something with these later per year\n",
    "        - hashing onto a dictionary should allow for ease of access since no 2 years will have the same EXACT one hot columns, hence the list\n",
    "    The above is deprecated, after merging with similar columns these will all be useless to us\n",
    "\n",
    "    We also drop the _Empty for EVERYTHING since that information is useless to us\n",
    "    \n",
    "    Input: frames dict{str: pd.DataFrames}\n",
    "    Ouput: None\n",
    "\n",
    "    https://stackoverflow.com/questions/45312377/how-to-one-hot-encode-from-a-pandas-column-containing-a-list\n",
    "\n",
    "    Rough example flow of function for one sample:\n",
    "    C; C++; Perl -> [C, C++, Perl] -> [1, 1, 1, 0]\n",
    "    Python       -> [Python]       -> [0, 0, 0, 1]\n",
    "    \"\"\"\n",
    "    # some constants\n",
    "    standard = [(\"language\", \"lg\"), (\"database\", \"db\"), (\"platform\", \"pf\"), (\"webframe\", \"wf\"), (\"misctech\", \"mt\")]\n",
    "    status = [(\"wanttoworkwith\", \"www\"), (\"haveworkedwith\", \"hww\")]\n",
    "    \n",
    "    for key, frame in frames.items():\n",
    "        new_cols = []\n",
    "        for stan, abv in standard:\n",
    "            for stat, abr in status:\n",
    "                coi = stan + stat # coi = column of interest\n",
    "                abbr = abv + abr + \"_\"\n",
    "                mlb = MultiLabelBinarizer(sparse_output=True) # saves ram\n",
    "                frame[coi] = frame[coi].str.split(\";\")\n",
    "                transformed = mlb.fit_transform(frame.pop(coi))\n",
    "                new_cois = [abbr + name for name in mlb.classes_]\n",
    "                frame = frame.join(\n",
    "                            pd.DataFrame.sparse.from_spmatrix(\n",
    "                                transformed,\n",
    "                                index=frame.index,\n",
    "                                columns=new_cois\n",
    "                            )\n",
    "                        )\n",
    "                new_cois.remove(abbr + \"Empty\")\n",
    "                new_cols += new_cois\n",
    "                frame = frame.drop(abbr + \"Empty\", axis=1)\n",
    "        # this needs to be here, if not throse Sparse type errors\n",
    "        # # Sparse types don't allow normal groupby operations (ie reshape) so we need to turn them into ints\n",
    "        # # int8 don't take up a ton and it's just 0's and 1's\n",
    "        # # for all intents and purposes these are sparse matrices, we just want to avoid the object\n",
    "        frame[new_cols] = frame[new_cols].fillna(0)\n",
    "        frame[new_cols] = frame[new_cols].astype('int8')\n",
    "        frames[key] = frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "137e10a2-2a22-44b4-a8e5-187adf8ff078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbr_education(frames: dict) -> None:\n",
    "    \"\"\"\n",
    "    Similar in spirit to the other one hots, but this is in place\n",
    "    Automatically abbreviates education levels across all frames\n",
    "    Had to hard code the list again, not a big deal only 8 items\n",
    "    \n",
    "    Input: frames dict{str: pd.DataFrames}\n",
    "    Ouput: None\n",
    "    \"\"\"\n",
    "    # more hardcoded stuff that are needed\n",
    "    abbreviations = [\"Associate's\", \"Bachelor's\", \"Master's\", \"Elementary\", \"Professional\", \"Secondary\", \"Some College\", \"Else\"]\n",
    "    \n",
    "    for key, frame in frames.items():\n",
    "        # easier to replace this, makes it much easier to work with\n",
    "        frame['edlevel'] = frame['edlevel'].replace({'I never completed any formal education': 'Something else'})\n",
    "\n",
    "        # need the sorted since they have the same rough scheme\n",
    "        levels = list(frame['edlevel'].unique())\n",
    "        levels.sort()\n",
    "        o = 0 # offset\n",
    "\n",
    "        # dictionary to feed into repalce function\n",
    "        replace_dict = {}\n",
    "        for i in range(len(levels)):\n",
    "            col = levels[i]\n",
    "            if col == 'nan':\n",
    "                break\n",
    "            abbr = abbreviations[i-o]\n",
    "            if 'doctoral' in col:\n",
    "                replace_dict[col] = \"Doctoral\"\n",
    "                o += 1\n",
    "                continue\n",
    "            replace_dict[col] = abbr\n",
    "                \n",
    "        frame['edlevel'] = frame['edlevel'].replace(replace_dict)\n",
    "        frames[key] = frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a2373aff-0984-49bf-95b6-58b161e4b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_ages(frames: dict) -> None:\n",
    "    \"\"\"\n",
    "    Bin ages so that they match with the later year surveys\n",
    "    \"\"\"\n",
    "    bins = [0, 18, 24, 34, 44, 54, 64, 100]\n",
    "    labels = ['Under 18 years old', '18-24 years old', '25-34 years old', '35-44 years old', '45-54 years old', '55-64 years old', '65 years or older']\n",
    "    for year, frame in frames.items():    \n",
    "        if frame[\"age\"].dtypes == float:\n",
    "            frame[\"age\"] = pd.cut(frame[\"age\"], bins=bins, labels=labels)\n",
    "        frame[\"age\"] = frame[\"age\"].astype('str')\n",
    "        \n",
    "        frames[year] = frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e779fe3c-7fb7-4057-a527-4412508de3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_col(frames) -> list:\n",
    "    \"\"\"\n",
    "    Returns the set of columns that the all share, ideally we maximize the ratio of this to merge.\n",
    "    \"\"\"\n",
    "    union = []\n",
    "    for key, frame in frames.items():\n",
    "        union.append(set(frame.columns))\n",
    "        \n",
    "    standard = union[0]\n",
    "    for cols in union[1:]:\n",
    "        standard = standard.intersection(cols)\n",
    "    return list(standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "49623a29-5d7b-4b33-b3e6-419a8a05cf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_devtype(df: pd.DataFrame) -> (pd.DataFrame, list):\n",
    "    \"\"\"\n",
    "    Standardizing DevType so that we can merge on ai-net data\n",
    "    \"\"\"\n",
    "    def map_job(category_list) -> list:\n",
    "        devtype = set()\n",
    "        for category in category_list:\n",
    "            if (clean := \"data scientist\") in category.lower():\n",
    "                devtype.add(clean)\n",
    "            elif \"math\" in category.lower() or \"stat\" in category.lower():\n",
    "                devtype.add(\"mathematician_statistician\")\n",
    "            elif (clean := \"analyst\") in category.lower():\n",
    "                devtype.add(clean)\n",
    "            elif (clean := \"manage\") in category.lower():\n",
    "                devtype.add(clean + \"ment\")\n",
    "            elif (clean := \"scientist\") in category.lower():\n",
    "                devtype.add(clean + \"_other\")\n",
    "            elif (clean := \"engineer\") in category.lower():\n",
    "                devtype.add(clean + \"_other\")\n",
    "            elif (clean := \"developer\") in category.lower():\n",
    "                devtype.add(clean)\n",
    "            else:\n",
    "                devtype.add(\"systems_architect\")\n",
    "        return list(devtype)\n",
    "        \n",
    "    coi = \"devtype\"\n",
    "    mlb = MultiLabelBinarizer(sparse_output=True) # saves ram\n",
    "    df[coi] = df[coi].str.split(\";\")\n",
    "    df[coi] = df[coi].apply(map_job)\n",
    "    transformed = mlb.fit_transform(df.pop(coi))\n",
    "    new_cols = mlb.classes_\n",
    "    df = df.join(\n",
    "                pd.DataFrame.sparse.from_spmatrix(\n",
    "                    transformed,\n",
    "                    index=df.index,\n",
    "                    columns=mlb.classes_\n",
    "                )\n",
    "            )\n",
    "    # see above binarizer\n",
    "    df[new_cols] = df[new_cols].fillna(0)\n",
    "    df[new_cols] = df[new_cols].astype('int8')\n",
    "    return df, new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c0bff165-ea33-45a6-a11b-6202f82e75a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stackoverflow() -> (pd.DataFrame, list, list):\n",
    "    \"\"\"\n",
    "    Reads CSVs and gets the numbe of data professionals. Any empty values are dropped from job title and \n",
    "    salary so we will always have data. Other columns may have nans.\n",
    "    Data Manipulation:\n",
    "    - dropping nans from salary and devtype combined\n",
    "    - Changing the salary column to ConvertedCompYearly so we can merge all data frames comes time\n",
    "    - Lowering column names since there was some weird camel case going on\n",
    "    - Converting specific columns that mean the same thing per year into a singular name\n",
    "    - Fill in nans for language/skill specific values with \"nan\"\n",
    "      - this is so we can one hot later on for a more concise analysis, more later on\n",
    "    - Binarize the different skills per year, see create_onehot_skills\n",
    "    - Next we abbreviate education levels so that we can also one hot them, see above\n",
    "    - Change education to keep into one column binarizing doesn't make any sense\n",
    "    - Changing org size into something much more manageable, mainly the I don't know field\n",
    "    - We merge them into one, the same groupby operations can still be done as if seperate\n",
    "    - Encode devtype to binarize as well since it's very difficult to parse through ; every single time\n",
    "        - we can do some clever work arounds\n",
    "    - Lastly we return the skills columns really quick to save headache later on\n",
    "\n",
    "    Inputs: Nothing\n",
    "    Outputs: tuple(pd.DataFrame, list[str], list[str])\n",
    "    \"\"\"\n",
    "    country_abbreviations_1 = {country.name: country.alpha_3 for country in pycountry.countries}\n",
    "    country_abbreviations_2 = {country.official_name: country.alpha_3 for country in pycountry.countries}\n",
    "    frames = {}\n",
    "    stack_o_files = os.listdir(\"data/stack_overflow/\")\n",
    "    for file in stack_o_files:\n",
    "        year = file[-8:-4]\n",
    "        df = pd.read_csv(f\"data/stack_overflow/{file}\", encoding='ISO-8859-1')\n",
    "\n",
    "        # standardize compensation columns\n",
    "        if 'ConvertedComp' in df.columns:\n",
    "            df = df.rename(columns={'ConvertedComp': 'ConvertedCompYearly'})\n",
    "\n",
    "        # standardize some columns\n",
    "        # using camel case resulted in errors with webframe where sometimes F was capitalized\n",
    "        standard = [\"language\", \"database\", \"platform\", \"webframe\", \"misctech\"]\n",
    "        df.columns = df.columns.str.lower()\n",
    "        for stan in standard:\n",
    "            if f\"{stan}workedwith\" in df.columns:\n",
    "                df = df.rename(columns={f'{stan}workedwith': f'{stan}haveworkedwith', f'{stan}desirenextyear':f'{stan}wanttoworkwith'})\n",
    "            df[f\"{stan}haveworkedwith\"] = df[f\"{stan}haveworkedwith\"].fillna(value=\"Empty\")\n",
    "            df[f\"{stan}wanttoworkwith\"] = df[f\"{stan}wanttoworkwith\"].fillna(value=\"Empty\")\n",
    "\n",
    "        # standardize some country names, now they should match with Kaggle dataset\n",
    "        df[\"country\"] = df[\"country\"].replace(country_abbreviations_1)\n",
    "        df[\"country\"] = df[\"country\"].replace(country_abbreviations_2)\n",
    "\n",
    "        # we have some numbers so we can't just do entire df\n",
    "        df[['edlevel', 'orgsize']] = df[['edlevel', 'orgsize']].fillna(value=\"nan\")\n",
    "        df['orgsize'] = df['orgsize'].replace({'I donÃ¢\\x80\\x99t know': 'IDK'})\n",
    "        \n",
    "        df = df.dropna(subset=[\"devtype\", \"convertedcompyearly\"])\n",
    "        df = df[df[\"devtype\"].str.contains(\"data\", case=False)]\n",
    "        df[\"count\"] = [1] * len(df) # this is for our groupby so that we can say count > cull when we sum or count\n",
    "        df[\"year\"] = [year] * len(df)\n",
    "        frames[f\"df_data_{year}\"] = df\n",
    "\n",
    "    # oops forgot indentation\n",
    "    abbr_education(frames)\n",
    "    bin_ages(frames)\n",
    "    create_onehot_skills(frames)\n",
    "    similar = find_similar_col(frames)\n",
    "    \n",
    "    # finally going to standardize to merge devtypes\n",
    "    for key, frame in frames.items():\n",
    "        frames[key] = frame[similar]\n",
    "    df = pd.concat([frame for key, frame in frames.items()], axis=0)\n",
    "    df, employment = encode_devtype(df)\n",
    "    skills = [col for col in df.columns if any(substr in col for substr in ['lg', 'db', 'pf', 'wf', 'mt'])]\n",
    "    return df, skills, employment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bc89d7-2beb-4326-8870-aef574acf581",
   "metadata": {},
   "source": [
    "## Reading (Run Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5b951d32-a9c9-4c3b-948a-d344290eccf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luism\\anaconda3\\envs\\milestone_1\\Lib\\site-packages\\pycountry\\db.py:51: UserWarning: Country's official_name not found. Country name provided instead.\n",
      "  warnings.warn(warning_message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "stack_overflow, skills_list, employments = read_stackoverflow()\n",
    "ppp = read_ppp()\n",
    "salaries = read_salaries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b18fd1-a915-4d36-948a-8562d376e6e5",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f84e2b-2d99-41b6-918d-b11c5474dc28",
   "metadata": {},
   "source": [
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb24dce-b10d-43ec-ac8f-b6fe1d025ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bf7c984-aaef-4f8c-8db3-195d6882e92d",
   "metadata": {},
   "source": [
    "### One Way ANOVA on Comp Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcee4264-7c3c-4eaf-b79d-cfba38fb5284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f53b3c1-8f41-47cc-ac4a-d31e956dcab2",
   "metadata": {},
   "source": [
    "### T-Tests On Salary if Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a666c9c-e2f5-42b8-b1c9-c91fa9f0af5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "008737cf-a49b-4247-b1b7-e643e20c5fc7",
   "metadata": {},
   "source": [
    "### One Way ANOVA on Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb9280b-de69-49a1-8028-768ef52188ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "626142fb-fcce-4d15-ac48-0d123d45ce65",
   "metadata": {},
   "source": [
    "### One Way ANOVA on Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f38343-39a0-452a-8d5b-b6dbba50940b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fedc712b-a45e-400c-a312-3589129564c2",
   "metadata": {},
   "source": [
    "### One Way ANOVA on Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37540393-3543-4c49-b7fa-9e8906f42bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44aed1d7-f2f9-490e-bed2-56e5ad3aac2f",
   "metadata": {},
   "source": [
    "### One Way ANOVA on Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e706ce4-0bf5-4292-915f-4d13f14468db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4f1616f-bb83-4905-b7be-2391b1a99972",
   "metadata": {},
   "source": [
    "### Salary Skew Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2777e6-290b-4d14-80f7-33c8fe15e748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb98f086-8574-48f4-a411-9ca546b79e66",
   "metadata": {},
   "source": [
    "### Identify any outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207e4216-9082-4e13-b9fd-809dca3fae46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "beacca4d-9626-45b1-b984-38bb1a581aa3",
   "metadata": {},
   "source": [
    "## Charts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe034f39-75a4-44f7-b4bb-9e26499790d8",
   "metadata": {},
   "source": [
    "### Choropleth Chart on Salary by Location (need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2dda1b-3619-40ae-93c9-40f0aa0cc65a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56666167-438b-4ed8-96f9-17c14a7faa3f",
   "metadata": {},
   "source": [
    "### Line Chart of Salary by Location (need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3163f2-78a1-4350-b853-ecbfd45db2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04d25be7-be5b-4181-9fbe-066671fc28c9",
   "metadata": {},
   "source": [
    "### Blox Plot of a Country to Show Skew if Any (need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49fa54e-1078-47f0-b8b7-0592e72e0a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42949cf8-8fd8-471f-baa7-2b4cb422df1f",
   "metadata": {},
   "source": [
    "### Histogram of Countries with Highest Response Rates (need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dcc478-7849-42eb-bbfa-78b350e47b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "199a4326-9392-462f-a1da-16c13a2a3f8b",
   "metadata": {},
   "source": [
    "### Sunburst Plot (want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c245f47-9304-49ac-8ed7-d82a29766b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
