{
<<<<<<< HEAD
 "cells": [
  {
   "cell_type": "markdown",
   "id": "460bcd30-1c2e-4756-ba10-bfe0a0747233",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0863b356-bcad-4fb1-8b8a-eb91a6016e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pycountry\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer\n",
    "import pingouin as pg\n",
    "# import warnings\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 500)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99d2e08-d067-414d-bf79-44143c6c6191",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "108206fc-d649-4506-8234-c0f970171086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ppp() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads PPP csv and returns resulting data frame\n",
    "    Data Manipulations:\n",
    "    - Only use years since we're not going to use anything before unless we go historic route\n",
    "    - Fill nans with string type Null\n",
    "    - - thought process is to us other functions that will detect str type and throw an error\n",
    "    - - if nan operation will probably go through, so doing isinstance == str would be best probably\n",
    "    - Also index columns are the country code, should match to the PyCountry library\n",
    "    \n",
    "    Inputs: None\n",
    "    Output: pd.DataFrame\n",
    "    \"\"\"\n",
    "    years = [\"2019\", \"2020\", \"2021\", \"2022\"]\n",
    "    ppp = pd.read_csv(\"data/ppp.csv\", header=2, index_col=\"Country Code\")[years]\n",
    "    # ppp = ppp.fillna(\"Null\") # this way we can control the type, so we can create a function that checks type before anything else\n",
    "    return ppp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b67c2165-6e83-49e2-9d9c-a16a05e99987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walrus_helper(salaries: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Helper function that is just a for loop that goes through unique job titles and assigns a basic name\n",
    "\n",
    "    Input: pd.DataFrame\n",
    "    Output: dict{str: str}\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for job in list(salaries[\"job_title\"].unique()):\n",
    "        if (short := \"Analyst\") in job:\n",
    "            mapping[job] = short.lower()\n",
    "    \n",
    "        elif (short := \"Engineer\") in job:\n",
    "            mapping[job] = short.lower() + \"_other\"\n",
    "            \n",
    "        elif (short := \"Data Scientist\") in job or \"Data Science\" in job:\n",
    "                mapping[job] = '_'.join(short.lower().split(\" \"))\n",
    "            \n",
    "        elif \"Architect\" in job:\n",
    "            mapping[job] = \"systems_architect\"\n",
    "    \n",
    "        elif \"Manager\" in job:\n",
    "            mapping[job] = \"management\"\n",
    "    \n",
    "        elif (short := \"Developer\") in job:\n",
    "            mapping[job] = short.lower()\n",
    "            \n",
    "        elif \"math\" in job.lower() or \"stat\" in job.lower():\n",
    "            mapping[job] = \"mathematician_statistician\"\n",
    "            \n",
    "        else:\n",
    "            mapping[job] = \"scientist_other\"\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "575b5e8d-2564-4f3d-9104-dbf8f747fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_salaries() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads the salaries from ai-net and returns them into a dataframe\n",
    "    Data Manipulation:\n",
    "    - Change 2 letter country names into 3 letter names for uniformity\n",
    "    - Map above function in job_title to simpler names\n",
    "    - Only taking 2020 - 2023, we have no data on 2024\n",
    "    \n",
    "    Input: None\n",
    "    Output: pd.DataFrame\n",
    "    \"\"\"\n",
    "    salaries = pd.read_csv(\"data/salaries.csv\")\n",
    "    country_abbreviations = {country.alpha_2: country.alpha_3 for country in pycountry.countries}\n",
    "    mapping = walrus_helper(salaries)\n",
    "    \n",
    "    salaries[[\"employee_residence\", \"company_location\"]] = salaries[[\"employee_residence\", \"company_location\"]].replace(country_abbreviations)\n",
    "    salaries[\"job_title\"] = salaries[\"job_title\"].replace(mapping)\n",
    "    salaries = salaries[salaries[\"work_year\"] < 2024]\n",
    "    \n",
    "    return salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "59d37b96-23a6-45d2-a40f-0813123c1395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_onehot_skills(frames: dict) -> None:\n",
    "    \"\"\"\n",
    "    Given a dictionary of pandas dataframes we want to one hot the skills in particular.\n",
    "    We want to take the skills in the different columns and one hot them such we can sum them for groupby operations.\n",
    "    We get a dictionary of pandas DataFrames and perform an inplace operation such that we don't have to create new memory.\n",
    "    Return a dictionary of a list of strings for a couple reasons:\n",
    "        - there's no way we will remember all of these so automation by putting these into a list seemed like the best idea\n",
    "        - the keys will match those in the input in case we want to do something with these later per year\n",
    "        - hashing onto a dictionary should allow for ease of access since no 2 years will have the same EXACT one hot columns, hence the list\n",
    "    The above is deprecated, after merging with similar columns these will all be useless to us\n",
    "\n",
    "    We also drop the _Empty for EVERYTHING since that information is useless to us\n",
    "    \n",
    "    Input: frames dict{str: pd.DataFrames}\n",
    "    Ouput: None\n",
    "\n",
    "    https://stackoverflow.com/questions/45312377/how-to-one-hot-encode-from-a-pandas-column-containing-a-list\n",
    "\n",
    "    Rough example flow of function for one sample:\n",
    "    C; C++; Perl -> [C, C++, Perl] -> [1, 1, 1, 0]\n",
    "    Python       -> [Python]       -> [0, 0, 0, 1]\n",
    "    \"\"\"\n",
    "    # some constants\n",
    "    standard = [(\"language\", \"lg\"), (\"database\", \"db\"), (\"platform\", \"pf\"), (\"webframe\", \"wf\"), (\"misctech\", \"mt\")]\n",
    "    status = [(\"wanttoworkwith\", \"www\"), (\"haveworkedwith\", \"hww\")]\n",
    "    \n",
    "    for key, frame in frames.items():\n",
    "        new_cols = []\n",
    "        for stan, abv in standard:\n",
    "            for stat, abr in status:\n",
    "                coi = stan + stat # coi = column of interest\n",
    "                abbr = abv + abr + \"_\"\n",
    "                mlb = MultiLabelBinarizer(sparse_output=True) # saves ram\n",
    "                frame[coi] = frame[coi].str.split(\";\")\n",
    "                transformed = mlb.fit_transform(frame.pop(coi))\n",
    "                new_cois = [abbr + name for name in mlb.classes_]\n",
    "                frame = frame.join(\n",
    "                            pd.DataFrame.sparse.from_spmatrix(\n",
    "                                transformed,\n",
    "                                index=frame.index,\n",
    "                                columns=new_cois\n",
    "                            )\n",
    "                        )\n",
    "                new_cois.remove(abbr + \"Empty\")\n",
    "                new_cols += new_cois\n",
    "                frame = frame.drop(abbr + \"Empty\", axis=1)\n",
    "        # this needs to be here, if not throse Sparse type errors\n",
    "        # # Sparse types don't allow normal groupby operations (ie reshape) so we need to turn them into ints\n",
    "        # # int8 don't take up a ton and it's just 0's and 1's\n",
    "        # # for all intents and purposes these are sparse matrices, we just want to avoid the object\n",
    "        frame[new_cols] = frame[new_cols].fillna(0)\n",
    "        frame[new_cols] = frame[new_cols].astype('int8')\n",
    "        frames[key] = frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "137e10a2-2a22-44b4-a8e5-187adf8ff078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbr_education(frames: dict) -> None:\n",
    "    \"\"\"\n",
    "    Similar in spirit to the other one hots, but this is in place\n",
    "    Automatically abbreviates education levels across all frames\n",
    "    Had to hard code the list again, not a big deal only 8 items\n",
    "    \n",
    "    Input: frames dict{str: pd.DataFrames}\n",
    "    Ouput: None\n",
    "    \"\"\"\n",
    "    # more hardcoded stuff that are needed\n",
    "    abbreviations = [\"Associate's\", \"Bachelor's\", \"Master's\", \"Elementary\", \"Professional\", \"Secondary\", \"Some College\", \"Else\"]\n",
    "    \n",
    "    for key, frame in frames.items():\n",
    "        # easier to replace this, makes it much easier to work with\n",
    "        frame['edlevel'] = frame['edlevel'].replace({'I never completed any formal education': 'Something else'})\n",
    "\n",
    "        # need the sorted since they have the same rough scheme\n",
    "        levels = list(frame['edlevel'].unique())\n",
    "        levels.sort()\n",
    "        o = 0 # offset\n",
    "\n",
    "        # dictionary to feed into repalce function\n",
    "        replace_dict = {}\n",
    "        for i in range(len(levels)):\n",
    "            col = levels[i]\n",
    "            if col == 'nan':\n",
    "                break\n",
    "            abbr = abbreviations[i-o]\n",
    "            if 'doctoral' in col:\n",
    "                replace_dict[col] = \"Doctoral\"\n",
    "                o += 1\n",
    "                continue\n",
    "            replace_dict[col] = abbr\n",
    "                \n",
    "        frame['edlevel'] = frame['edlevel'].replace(replace_dict)\n",
    "        frames[key] = frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a2373aff-0984-49bf-95b6-58b161e4b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_ages(frames: dict) -> None:\n",
    "    \"\"\"\n",
    "    Bin ages so that they match with the later year surveys\n",
    "    \"\"\"\n",
    "    bins = [0, 18, 24, 34, 44, 54, 64, 100]\n",
    "    labels = ['Under 18 years old', '18-24 years old', '25-34 years old', '35-44 years old', '45-54 years old', '55-64 years old', '65 years or older']\n",
    "    for year, frame in frames.items():    \n",
    "        if frame[\"age\"].dtypes == float:\n",
    "            frame[\"age\"] = pd.cut(frame[\"age\"], bins=bins, labels=labels)\n",
    "        frame[\"age\"] = frame[\"age\"].astype('str')\n",
    "        \n",
    "        frames[year] = frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e779fe3c-7fb7-4057-a527-4412508de3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_col(frames) -> list:\n",
    "    \"\"\"\n",
    "    Returns the set of columns that the all share, ideally we maximize the ratio of this to merge.\n",
    "    \"\"\"\n",
    "    union = []\n",
    "    for key, frame in frames.items():\n",
    "        union.append(set(frame.columns))\n",
    "        \n",
    "    standard = union[0]\n",
    "    for cols in union[1:]:\n",
    "        standard = standard.intersection(cols)\n",
    "    return list(standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "49623a29-5d7b-4b33-b3e6-419a8a05cf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_devtype(df: pd.DataFrame) -> (pd.DataFrame, list):\n",
    "    \"\"\"\n",
    "    Standardizing DevType so that we can merge on ai-net data\n",
    "    \"\"\"\n",
    "    def map_job(category_list) -> list:\n",
    "        devtype = set()\n",
    "        for category in category_list:\n",
    "            if (clean := \"data scientist\") in category.lower():\n",
    "                devtype.add(clean)\n",
    "            elif \"math\" in category.lower() or \"stat\" in category.lower():\n",
    "                devtype.add(\"mathematician_statistician\")\n",
    "            elif (clean := \"analyst\") in category.lower():\n",
    "                devtype.add(clean)\n",
    "            elif (clean := \"manage\") in category.lower():\n",
    "                devtype.add(clean + \"ment\")\n",
    "            elif (clean := \"scientist\") in category.lower():\n",
    "                devtype.add(clean + \"_other\")\n",
    "            elif (clean := \"engineer\") in category.lower():\n",
    "                devtype.add(clean + \"_other\")\n",
    "            elif (clean := \"developer\") in category.lower():\n",
    "                devtype.add(clean)\n",
    "            else:\n",
    "                devtype.add(\"systems_architect\")\n",
    "        return list(devtype)\n",
    "        \n",
    "    coi = \"devtype\"\n",
    "    mlb = MultiLabelBinarizer(sparse_output=True) # saves ram\n",
    "    df[coi] = df[coi].str.split(\";\")\n",
    "    df[coi] = df[coi].apply(map_job)\n",
    "    transformed = mlb.fit_transform(df.pop(coi))\n",
    "    new_cols = mlb.classes_\n",
    "    df = df.join(\n",
    "                pd.DataFrame.sparse.from_spmatrix(\n",
    "                    transformed,\n",
    "                    index=df.index,\n",
    "                    columns=mlb.classes_\n",
    "                )\n",
    "            )\n",
    "    # see above binarizer\n",
    "    df[new_cols] = df[new_cols].fillna(0)\n",
    "    df[new_cols] = df[new_cols].astype('int8')\n",
    "    return df, new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c0bff165-ea33-45a6-a11b-6202f82e75a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stackoverflow() -> (pd.DataFrame, list, list):\n",
    "    \"\"\"\n",
    "    Reads CSVs and gets the numbe of data professionals. Any empty values are dropped from job title and \n",
    "    salary so we will always have data. Other columns may have nans.\n",
    "    Data Manipulation:\n",
    "    - dropping nans from salary and devtype combined\n",
    "    - Changing the salary column to ConvertedCompYearly so we can merge all data frames comes time\n",
    "    - Lowering column names since there was some weird camel case going on\n",
    "    - Converting specific columns that mean the same thing per year into a singular name\n",
    "    - Fill in nans for language/skill specific values with \"nan\"\n",
    "      - this is so we can one hot later on for a more concise analysis, more later on\n",
    "    - Binarize the different skills per year, see create_onehot_skills\n",
    "    - Next we abbreviate education levels so that we can also one hot them, see above\n",
    "    - Change education to keep into one column binarizing doesn't make any sense\n",
    "    - Changing org size into something much more manageable, mainly the I don't know field\n",
    "    - We merge them into one, the same groupby operations can still be done as if seperate\n",
    "    - Encode devtype to binarize as well since it's very difficult to parse through ; every single time\n",
    "        - we can do some clever work arounds\n",
    "    - Lastly we return the skills columns really quick to save headache later on\n",
    "\n",
    "    Inputs: Nothing\n",
    "    Outputs: tuple(pd.DataFrame, list[str], list[str])\n",
    "    \"\"\"\n",
    "    country_abbreviations_1 = {country.name: country.alpha_3 for country in pycountry.countries}\n",
    "    country_abbreviations_2 = {country.official_name: country.alpha_3 for country in pycountry.countries}\n",
    "    frames = {}\n",
    "    stack_o_files = os.listdir(\"data/stack_overflow/\")\n",
    "    for file in stack_o_files:\n",
    "        year = file[-8:-4]\n",
    "        df = pd.read_csv(f\"data/stack_overflow/{file}\", encoding='ISO-8859-1')\n",
    "\n",
    "        # standardize compensation columns\n",
    "        if 'ConvertedComp' in df.columns:\n",
    "            df = df.rename(columns={'ConvertedComp': 'ConvertedCompYearly'})\n",
    "\n",
    "        # standardize some columns\n",
    "        # using camel case resulted in errors with webframe where sometimes F was capitalized\n",
    "        standard = [\"language\", \"database\", \"platform\", \"webframe\", \"misctech\"]\n",
    "        df.columns = df.columns.str.lower()\n",
    "        for stan in standard:\n",
    "            if f\"{stan}workedwith\" in df.columns:\n",
    "                df = df.rename(columns={f'{stan}workedwith': f'{stan}haveworkedwith', f'{stan}desirenextyear':f'{stan}wanttoworkwith'})\n",
    "            df[f\"{stan}haveworkedwith\"] = df[f\"{stan}haveworkedwith\"].fillna(value=\"Empty\")\n",
    "            df[f\"{stan}wanttoworkwith\"] = df[f\"{stan}wanttoworkwith\"].fillna(value=\"Empty\")\n",
    "\n",
    "        # standardize some country names, now they should match with Kaggle dataset\n",
    "        df[\"country\"] = df[\"country\"].replace(country_abbreviations_1)\n",
    "        df[\"country\"] = df[\"country\"].replace(country_abbreviations_2)\n",
    "\n",
    "        # we have some numbers so we can't just do entire df\n",
    "        df[['edlevel', 'orgsize']] = df[['edlevel', 'orgsize']].fillna(value=\"nan\")\n",
    "        df['orgsize'] = df['orgsize'].replace({'I donâ\\x80\\x99t know': 'IDK'})\n",
    "        \n",
    "        df = df.dropna(subset=[\"devtype\", \"convertedcompyearly\"])\n",
    "        df = df[df[\"devtype\"].str.contains(\"data\", case=False)]\n",
    "        df[\"count\"] = [1] * len(df) # this is for our groupby so that we can say count > cull when we sum or count\n",
    "        df[\"year\"] = [year] * len(df)\n",
    "        frames[f\"df_data_{year}\"] = df\n",
    "\n",
    "    # oops forgot indentation\n",
    "    abbr_education(frames)\n",
    "    bin_ages(frames)\n",
    "    create_onehot_skills(frames)\n",
    "    similar = find_similar_col(frames)\n",
    "    \n",
    "    # finally going to standardize to merge devtypes\n",
    "    for key, frame in frames.items():\n",
    "        frames[key] = frame[similar]\n",
    "    df = pd.concat([frame for key, frame in frames.items()], axis=0)\n",
    "    df, employment = encode_devtype(df)\n",
    "    skills = [col for col in df.columns if any(substr in col for substr in ['lg', 'db', 'pf', 'wf', 'mt'])]\n",
    "    return df, skills, employment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bc89d7-2beb-4326-8870-aef574acf581",
   "metadata": {},
   "source": [
    "## Reading (Run Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5b951d32-a9c9-4c3b-948a-d344290eccf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luism\\anaconda3\\envs\\milestone_1\\Lib\\site-packages\\pycountry\\db.py:51: UserWarning: Country's official_name not found. Country name provided instead.\n",
      "  warnings.warn(warning_message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "stack_overflow, skills_list, employments = read_stackoverflow()\n",
    "ppp = read_ppp()\n",
    "salaries = read_salaries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b18fd1-a915-4d36-948a-8562d376e6e5",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4016b093-f682-4ee0-b17a-bf3ee6fca86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yearscode',\n",
       " 'edlevel',\n",
       " 'comptotal',\n",
       " 'country',\n",
       " 'employment',\n",
       " 'convertedcompyearly',\n",
       " 'mainbranch',\n",
       " 'sopartfreq',\n",
       " 'sovisitfreq',\n",
       " 'age',\n",
       " 'soaccount',\n",
       " 'year',\n",
       " 'yearscodepro',\n",
       " 'count',\n",
       " 'socomm',\n",
       " 'surveyease',\n",
       " 'orgsize',\n",
       " 'surveylength',\n",
       " 'analyst',\n",
       " 'data scientist',\n",
       " 'developer',\n",
       " 'engineer_other',\n",
       " 'management',\n",
       " 'scientist_other',\n",
       " 'systems_architect']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more workable until we go by skills\n",
    "s_o = stack_overflow.drop(skills_list, axis=1)\n",
    "list(s_o.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b90d3082-b2f5-4616-888b-786c942e232f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['work_year', 'experience_level', 'employment_type', 'job_title',\n",
       "       'salary', 'salary_currency', 'salary_in_usd', 'employee_residence',\n",
       "       'remote_ratio', 'company_location', 'company_size'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salaries.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f84e2b-2d99-41b6-918d-b11c5474dc28",
   "metadata": {},
   "source": [
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb24dce-b10d-43ec-ac8f-b6fe1d025ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bf7c984-aaef-4f8c-8db3-195d6882e92d",
   "metadata": {},
   "source": [
    "### One Way ANOVA on Location and Comp Salary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e63c7d-fd26-4ae2-9c2c-cb276dc8b9ad",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f3e8a60d-0f33-4cd4-ae7c-ad8a8dbdb587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_countries(df: pd.DataFrame, want: int = 10) -> list:\n",
    "    \"\"\"\n",
    "    Get the number of countries that we want so we can index for ANOVA\n",
    "    Consistent across the years for a good comparison\n",
    "    \"\"\"\n",
    "    groupped = df.groupby('year')\n",
    "    loc_list = list()\n",
    "    head = 1\n",
    "    \n",
    "    while len(loc_list) < want:\n",
    "        loc_set = set()\n",
    "        for year, frame in groupped:\n",
    "            grouped = frame.groupby(\"country\").size().sort_values(ascending=False)\n",
    "            if not loc_set:\n",
    "                loc_set = set(grouped.head(head).index)\n",
    "                continue\n",
    "            temp_set = set(grouped.head(head).index)\n",
    "            loc_set = loc_set.intersection(temp_set)\n",
    "        loc_list = list(loc_set)\n",
    "        head += 1\n",
    "    return loc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a685da-148b-4f62-ae44-637375865b43",
   "metadata": {},
   "source": [
    "#### ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "dcee4264-7c3c-4eaf-b79d-cfba38fb5284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(year                    object\n",
       " country                 object\n",
       " convertedcompyearly    float64\n",
       " dtype: object,\n",
       " year                    int64\n",
       " country                object\n",
       " convertedcompyearly     int64\n",
       " dtype: object)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_so = s_o[[\"year\", \"country\", \"convertedcompyearly\"]]\n",
    "loc_so.loc[:, \"year\"] = loc_so.loc[:, \"year\"].astype('int64')\n",
    "\n",
    "loc_sal = salaries[[\"work_year\", \"employee_residence\", \"salary_in_usd\"]]\n",
    "loc_sal.loc[:, \"salary_in_usd\"] = loc_sal.loc[:, \"salary_in_usd\"].astype(\"float64\")\n",
    "loc_sal.columns = loc_so.columns\n",
    "\n",
    "\n",
    "location = pd.concat([loc_so, loc_sal], ignore_index=True)\n",
    "location\n",
    "loc_so.dtypes, loc_sal.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "38f7d1a7-7391-4acc-bd6d-e49451e50c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = get_similar_countries(location, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "45010bf2-51fa-418c-a654-f4086bc1f731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n"
     ]
    }
   ],
   "source": [
    "for year, frame in groupped:\n",
    "    print(year)\n",
    "    frame = frame[frame[\"country\"]]\n",
    "    aov = pg.anova(data=location, dv=\"convertedcompyearly\", between=\"country\", detailed=True)\n",
    "    display(aov)\n",
    "    grouped = frame.groupby(\"country\").size().sort_values(ascending=False).reset_index()\n",
    "    grouped = grouped[grouped[\"country\"].isin(loc_list)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cf1b53-2bae-4036-ab27-6d97ddd93786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f59e66-9525-4654-ab16-4abb7c5f806c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee37e699-5609-4d3f-8e37-c380a3193f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a641f84-248e-4f85-b397-864b3e370aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f53b3c1-8f41-47cc-ac4a-d31e956dcab2",
   "metadata": {},
   "source": [
    "### T-Tests On Salary if Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a666c9c-e2f5-42b8-b1c9-c91fa9f0af5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d7dbfe-48b2-45f3-b5bd-93d3c2405c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92080be9-e56c-4278-82c6-3f667ad16424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3eea8f-03f0-462f-a8d9-3e977a2ccae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc13bb-b35b-4c97-bb76-3bd2c0195de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "008737cf-a49b-4247-b1b7-e643e20c5fc7",
   "metadata": {},
   "source": [
    "### One Way ANOVA on Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb9280b-de69-49a1-8028-768ef52188ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc5f950-0548-444b-9b62-89bd7dde5fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092c903f-ec3d-43e6-9f7f-fcda812a0f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810da25a-9a62-4e4a-a8f1-390a6a7280b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a14b2b0-174e-40ff-817e-ce28e900ac0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "626142fb-fcce-4d15-ac48-0d123d45ce65",
   "metadata": {},
   "source": [
    "### One Way ANOVA on Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f38343-39a0-452a-8d5b-b6dbba50940b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f643f2-ff51-411b-ba79-4fdc05885fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c778fa-12b8-4a1f-b390-915a12280670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d27d796-bbe3-4ded-82a5-3bb3c766b8eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23bee4a-9a54-4da1-a243-ddc953ed1733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fedc712b-a45e-400c-a312-3589129564c2",
   "metadata": {},
   "source": [
    "### One Way ANOVA on Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37540393-3543-4c49-b7fa-9e8906f42bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb7f456-905c-4bca-ad5a-602524d58f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac430df-5cc0-4ab8-bb35-38b6c6b8e9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb9b9a3-c0cc-461d-b58e-997abad886d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cba8c5-d2f2-4212-8f25-57fb632908dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ba6f1a-06ac-4414-ba6a-c0f1e75864aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44aed1d7-f2f9-490e-bed2-56e5ad3aac2f",
   "metadata": {},
   "source": [
    "### One Way ANOVA on Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e706ce4-0bf5-4292-915f-4d13f14468db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8959f570-7546-4262-8ad8-aa8890f3fa31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da5f549-b03c-42ff-91c5-ff117d342854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff1df9b-9508-4720-84d0-b01ea9981088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10bd250-2701-4343-8675-3403d3ee80d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4f1616f-bb83-4905-b7be-2391b1a99972",
   "metadata": {},
   "source": [
    "### Salary Skew Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2777e6-290b-4d14-80f7-33c8fe15e748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba9015-35d6-47ca-9eb7-9a3f9f5bff8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb04dc1-0364-4337-af1e-577f7c17a24f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bac39e0-be19-4a9e-bad4-19ddbcacf375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94415ba-18a7-4f3c-8346-03185a9e2405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baf6a4a-45c7-4b9c-977a-1ee0c35cb6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb98f086-8574-48f4-a411-9ca546b79e66",
   "metadata": {},
   "source": [
    "### Identify any outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207e4216-9082-4e13-b9fd-809dca3fae46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccfa5ec-19eb-4068-9b59-b1bc410e1bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c0af5b-62e7-4b06-aa8c-1ebc9f7961d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987c1598-30ad-4345-9341-a07d3a596fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f658e7-a58b-42d4-bc66-e01a2145fa31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54008c6d-fdb5-45a2-bba0-17a7f87750de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "beacca4d-9626-45b1-b984-38bb1a581aa3",
   "metadata": {},
   "source": [
    "## Charts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe034f39-75a4-44f7-b4bb-9e26499790d8",
   "metadata": {},
   "source": [
    "### Choropleth Chart on Salary by Location (need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2dda1b-3619-40ae-93c9-40f0aa0cc65a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56666167-438b-4ed8-96f9-17c14a7faa3f",
   "metadata": {},
   "source": [
    "### Line Chart of Salary by Location (need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3163f2-78a1-4350-b853-ecbfd45db2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04d25be7-be5b-4181-9fbe-066671fc28c9",
   "metadata": {},
   "source": [
    "### Blox Plot of a Country to Show Skew if Any (need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49fa54e-1078-47f0-b8b7-0592e72e0a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42949cf8-8fd8-471f-baa7-2b4cb422df1f",
   "metadata": {},
   "source": [
    "### Histogram of Countries with Highest Response Rates (need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dcc478-7849-42eb-bbfa-78b350e47b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "199a4326-9392-462f-a1da-16c13a2a3f8b",
   "metadata": {},
   "source": [
    "### Sunburst Plot (want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c245f47-9304-49ac-8ed7-d82a29766b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
=======
 "cells": [],
 "metadata": {},
>>>>>>> 248683a8cd05fc101d6a44cafdd17201e914f2b2
 "nbformat": 4,
 "nbformat_minor": 5
}
