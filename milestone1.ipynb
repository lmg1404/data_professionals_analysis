{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f55912e4-edcc-4792-b84f-f9a39975ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pycountry\n",
    "pd.set_option('display.max_columns', 500)\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "059736ef-2164-4596-98e8-f365110949d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luism\\anaconda3\\envs\\milestone_1\\Lib\\site-packages\\pycountry\\db.py:51: UserWarning: Country's official_name not found. Country name provided instead.\n",
      "  warnings.warn(warning_message, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ppp.csv', 'salaries.csv', 'stack_overflow']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_overflow_files = (os.listdir(\"data/\"))\n",
    "# not worth going from 2011-2014. No data scientists.\n",
    "# ok, so decision to do (2019 maybe) 2020-2023 for analysis\n",
    "\n",
    "# GPT gave me this idea instead of going through every possible country manually\n",
    "country_abbreviations_1 = {country.name: country.alpha_2 for country in pycountry.countries}\n",
    "country_abbreviations_2 = {country.name: country.official_name for country in pycountry.countries}\n",
    "os.listdir(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10e6e980-8b30-4feb-872e-64cceaa62a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stackoverflow() -> dict:\n",
    "    \"\"\"\n",
    "    Reads CSVs and gets the numbe of data professionals. Any empty values are dropped from job title and \n",
    "    salary so we will always have data. Other columns may have nans.\n",
    "    Data Manipulation:\n",
    "    - dropping nans from salary and devtype combined\n",
    "    - Changing the salary column to ConvertedCompYearly so we can merge all data frames comes time\n",
    "    - Lowering column names since there was some weird camel case going on\n",
    "    - Converting specific columns that mean the same thing per year into a singular name\n",
    "    - Fill in nans for language/skill specific values with \"Empty\"\n",
    "      - this is so we can one hot later on for a more concise analysis, more later on\n",
    "    \"\"\"\n",
    "    frames = {}\n",
    "    stack_o_files = os.listdir(\"data/stack_overflow/\")\n",
    "    for file in stack_o_files:\n",
    "        year = file[-8:-4]\n",
    "        df = pd.read_csv(f\"data/stack_overflow/{file}\", encoding='ISO-8859-1')\n",
    "\n",
    "        # standardize compensation columns\n",
    "        if 'ConvertedComp' in df.columns:\n",
    "            df = df.rename(columns={'ConvertedComp': 'ConvertedCompYearly'})\n",
    "\n",
    "        # standardize some columns\n",
    "        # using camel case resulted in errors with webframe where sometimes F was capitalized\n",
    "        standard = [\"language\", \"database\", \"platform\", \"webframe\", \"misctech\"]\n",
    "        df.columns = df.columns.str.lower()\n",
    "        for stan in standard:\n",
    "            if f\"{stan}workedwith\" in df.columns:\n",
    "                df = df.rename(columns={f'{stan}workedwith': f'{stan}haveworkedwith', f'{stan}desirenextyear':f'{stan}wanttoworkwith'})\n",
    "            df[f\"{stan}haveworkedwith\"] = df[f\"{stan}haveworkedwith\"].fillna(value=\"Empty\")\n",
    "            df[f\"{stan}wanttoworkwith\"] = df[f\"{stan}wanttoworkwith\"].fillna(value=\"Empty\")\n",
    "\n",
    "        # standardize some country names, now they should match with Kaggle dataset\n",
    "        df[\"country\"] = df[\"country\"].replace(country_abbreviations_1)\n",
    "        df[\"country\"] = df[\"country\"].replace(country_abbreviations_2)\n",
    "        \n",
    "        \n",
    "        df = df.dropna(subset=[\"devtype\", \"convertedcompyearly\"])\n",
    "        df = df[df[\"devtype\"].str.contains(\"data\", case=False)]\n",
    "        df[\"year\"] = [year] * len(df)\n",
    "        frames[f\"df_data_{year}\"] = df\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79449a31-8bd9-4b64-b93f-0deedb21f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_dict = read_stackoverflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44403370-fb7b-4679-ac9b-856cf16b4db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_data_2019\t13393\t[]\n",
      "df_data_2020\t8294\t[]\n",
      "df_data_2021\t9272\t[]\n",
      "df_data_2022\t6921\t[]\n",
      "df_data_2023\t2480\t[]\n"
     ]
    }
   ],
   "source": [
    "# this is the number of entries we are working with in our frames\n",
    "# seeing how to standardize the columns some more\n",
    "\n",
    "query = \"Web\"\n",
    "for key, frame in frames_dict.items():\n",
    "    lang = []\n",
    "    for col in frame.columns:\n",
    "        lang.append(col) if query in col else None\n",
    "    print(f\"{key}\\t{len(frame)}\\t{lang}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8b6973f-16be-4d0e-a9a2-543078294b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(frames_dict[\"df_data_2019\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89e54b52-86b0-4468-a526-33b7dabc5db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(frames_dict[\"df_data_2020\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5382c8f8-589e-4817-8762-56ae2368768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(frames_dict[\"df_data_2021\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e954c060-4e0c-47b6-b32f-fd206ad8ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(frames_dict[\"df_data_2022\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47802c31-8420-41b5-8c3b-870494d64683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(frames_dict[\"df_data_2023\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "273a278b-e4c9-4ee9-8ad2-588f9757d841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do they have similar columns?\n",
    "def find_similar_col(frames) -> set:\n",
    "    \"\"\"\n",
    "    Returns the set of columns that the all share, ideally we maximize the ratio of this to merge.\n",
    "    \"\"\"\n",
    "    union = []\n",
    "    for key, frame in frames.items():\n",
    "        union.append(set(frame.columns))\n",
    "        \n",
    "    standard = union[0]\n",
    "    for cols in union[1:]:\n",
    "        standard = standard.intersection(cols)\n",
    "    return standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73d86b23-42fa-47a5-95cb-e1c00f86f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_similar_col(frames_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "add4430f-be36-47eb-aed9-ff0b9a619e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_data_2019: 83\n",
      "    max: US, 3856\n",
      "    min: AM, 11\n",
      "df_data_2020: 69\n",
      "    max: US, 2081\n",
      "    min: BY, 11\n",
      "df_data_2021: 70\n",
      "    max: United States of America, 2144\n",
      "    min: BA, 11\n",
      "df_data_2022: 63\n",
      "    max: United States of America, 1702\n",
      "    min: EG, 11\n",
      "df_data_2023: 36\n",
      "    max: United States of America, 687\n",
      "    min: CN, 11\n"
     ]
    }
   ],
   "source": [
    "# play around with the number and see if this is the spread that we want\n",
    "for key, frame in frames_dict.items():\n",
    "    grouped = (frame.groupby(\"country\").count())\n",
    "    grouped = grouped[grouped[\"mainbranch\"] > 10]\n",
    "    length = len(grouped)\n",
    "    print(f\"\"\"{key}: {length}\n",
    "    max: {grouped['mainbranch'].idxmax()}, {grouped['mainbranch'].max()}\n",
    "    min: {grouped['mainbranch'].idxmin()}, {grouped['mainbranch'].min()}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c637099f-3869-4d84-97c5-89ddcc9bd3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do they have similar columns?\n",
    "def find_similar_country(frames: dict, cull_factor=20) -> set:\n",
    "    \"\"\"\n",
    "    Given a particular minimum (cull_factor) find the countries in common among\n",
    "    frames.\n",
    "    \"\"\"\n",
    "    union = []\n",
    "    for key, frame in frames.items():\n",
    "        grouped = frame.groupby(\"country\").count()\n",
    "        grouped = grouped[grouped[\"mainbranch\"] > cull_factor]\n",
    "        union.append(set(grouped.index))\n",
    "        \n",
    "    standard = union[0]\n",
    "    for cols in union[1:]:\n",
    "        standard = standard.intersection(cols)\n",
    "    return standard\n",
    "\n",
    "def show_country_dist(frames: dict, countries: list, cull_factor: int) -> None:\n",
    "    \"\"\"\n",
    "    Just plot a bar chart for our country distributions using the above function.\n",
    "    \"\"\"\n",
    "    rows = len(frames)//2 + 1\n",
    "    fig, axes = plt.subplots(nrows=rows, ncols=2, figsize=(15,15))\n",
    "    fig.suptitle(f\"{len(countries)} respondents consistent across surveys greater than {cull_factor} responses\")\n",
    "    for (key, frame), ax in zip(frames.items(), axes.reshape(-1)):\n",
    "        grouped = frame.groupby(\"country\").count()\n",
    "        grouped = grouped.loc[list(countries)].sort_values(\"mainbranch\")\n",
    "        grouped.plot(y=\"mainbranch\", ax=ax, kind=\"bar\", legend=False)\n",
    "        ax.set_title(key[-4:])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b1e1d0c-5b5d-4f78-a0aa-460668f729f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# across all data sets here are the countries that are here most often\n",
    "# where is US? UK? They have different, inconsistent names throughout the years\n",
    "# # i.e. United States vs United States of America; UK vs United Kingdom, see above mapping\n",
    "cull_factor = 20\n",
    "country_sim = find_similar_country(frames_dict, cull_factor)\n",
    "# show_country_dist(frames_dict, list(country_sim), cull_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db8bc2c8-0f68-4509-aa9e-6b181b835b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['respondent', 'mainbranch', 'hobbyist', 'opensourcer', 'opensource',\n",
       "       'employment', 'country', 'student', 'edlevel', 'undergradmajor',\n",
       "       'eduother', 'orgsize', 'devtype', 'yearscode', 'age1stcode',\n",
       "       'yearscodepro', 'careersat', 'jobsat', 'mgridiot', 'mgrmoney',\n",
       "       'mgrwant', 'jobseek', 'lasthiredate', 'lastint', 'fizzbuzz',\n",
       "       'jobfactors', 'resumeupdate', 'currencysymbol', 'currencydesc',\n",
       "       'comptotal', 'compfreq', 'convertedcompyearly', 'workweekhrs',\n",
       "       'workplan', 'workchallenge', 'workremote', 'workloc', 'impsyn',\n",
       "       'coderev', 'coderevhrs', 'unittests', 'purchasehow', 'purchasewhat',\n",
       "       'languagehaveworkedwith', 'languagewanttoworkwith',\n",
       "       'databasehaveworkedwith', 'databasewanttoworkwith',\n",
       "       'platformhaveworkedwith', 'platformwanttoworkwith',\n",
       "       'webframehaveworkedwith', 'webframewanttoworkwith',\n",
       "       'misctechhaveworkedwith', 'misctechwanttoworkwith', 'devenviron',\n",
       "       'opsys', 'containers', 'blockchainorg', 'blockchainis', 'betterlife',\n",
       "       'itperson', 'offon', 'socialmedia', 'extraversion', 'screenname',\n",
       "       'sovisit1st', 'sovisitfreq', 'sovisitto', 'sofindanswer', 'sotimesaved',\n",
       "       'sohowmuchtime', 'soaccount', 'sopartfreq', 'sojobs', 'entteams',\n",
       "       'socomm', 'welcomechange', 'sonewcontent', 'age', 'gender', 'trans',\n",
       "       'sexuality', 'ethnicity', 'dependents', 'surveylength', 'surveyease',\n",
       "       'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2019 = frames_dict[\"df_data_2019\"]\n",
    "df_2019.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b308f2b-2eeb-4ac0-aeaf-6711431064f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_data_2019\n",
      "language: 0 ['HTML/CSS;JavaScript;Python']\n",
      "database: 0 ['MongoDB;PostgreSQL;SQLite;Other(s):']\n",
      "platform: 0 ['Empty']\n",
      "webframe: 0 ['Angular/Angular.js;jQuery']\n",
      "misctech: 0 ['.NET;.NET Core;TensorFlow;Unity 3D']\n",
      "\n",
      "df_data_2020\n",
      "language: 0 ['HTML/CSS;Java;JavaScript;Python;SQL']\n",
      "database: 0 ['MySQL']\n",
      "platform: 0 ['Google Cloud Platform;Linux']\n",
      "webframe: 0 ['Angular;Django']\n",
      "misctech: 0 ['Node.js']\n",
      "\n",
      "df_data_2021\n",
      "language: 0 ['C#;Python;SQL']\n",
      "database: 0 ['Empty']\n",
      "platform: 0 ['AWS']\n",
      "webframe: 0 ['Empty']\n",
      "misctech: 0 ['.NET Framework;.NET Core / .NET 5']\n",
      "\n",
      "df_data_2022\n",
      "language: 0 ['Bash/Shell;Haskell;Python;Rust']\n",
      "database: 0 ['Empty']\n",
      "platform: 0 ['DigitalOcean']\n",
      "webframe: 0 ['Django']\n",
      "misctech: 0 ['Electron;Keras;NumPy;Pandas;React Native;Scikit-learn;TensorFlow;Torch/PyTorch']\n",
      "\n",
      "df_data_2023\n",
      "language: 0 ['C;Python;Rust']\n",
      "database: 0 ['Elasticsearch;PostgreSQL;Redis']\n",
      "platform: 0 ['Empty']\n",
      "webframe: 0 ['Next.js;Node.js;Nuxt.js;Qwik;React;Remix;Solid.js;Vue.js']\n",
      "misctech: 0 ['Apache Spark;NumPy;Pandas;Scikit-Learn']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# basically with every one of these is separated by a ;\n",
    "# goal of next function:\n",
    "# # find the sub-strings separated by ;, nans will have to be replaced by \"None\" or \"Empty\"\n",
    "# # one hot the entries for example, if C appears in one of these queries, for that particular\n",
    "# # subject there will be a 1 for yes and 0 for no essentially\n",
    "# # this is why we need the None/Empty so we can add them up\n",
    "# # Eventually after one hotting we drop the None/Empty since it's a dummy column\n",
    "# # we would then be able to add them up using count or something and put onto a graph/analysis\n",
    "standard = [\"language\", \"database\", \"platform\", \"webframe\", \"misctech\"]\n",
    "want = \"wanttoworkwith\"\n",
    "have = \"haveworkedwith\"\n",
    "for key, frame in frames_dict.items():\n",
    "    print(key)\n",
    "    for stan in standard:\n",
    "        print(f\"{stan}: {frame[stan + want].isna().sum()} {frame[stan + want].sample(n=1).values}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc1150b-2867-4d8f-aaa6-552cd78e92ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5204937d-79cd-43c9-9638-1f86471d7843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e28ddc-0be6-478e-8283-b37560184413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dfef32-a021-4769-8cfe-d57140b247e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a6427-f65e-49e5-a73e-57c91a53e40c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef1474d-4069-4df1-b543-86c0d1cb275e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
