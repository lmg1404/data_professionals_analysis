{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f55912e4-edcc-4792-b84f-f9a39975ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pycountry\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "059736ef-2164-4596-98e8-f365110949d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ppp.csv', 'salaries.csv', 'stack_overflow']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_overflow_files = (os.listdir(\"data/\"))\n",
    "# not worth going from 2011-2014. No data scientists.\n",
    "# ok, so decision to do (2019 maybe) 2020-2023 for analysis\n",
    "\n",
    "# GPT gave me this idea instead of going through every possible country manually\n",
    "country_abbreviations_1 = {country.name: country.alpha_2 for country in pycountry.countries}\n",
    "country_abbreviations_2 = {country.name: country.official_name for country in pycountry.countries}\n",
    "os.listdir(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10e6e980-8b30-4feb-872e-64cceaa62a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stackoverflow_csvs() -> dict:\n",
    "    \"\"\"\n",
    "    Reads CSVs and gets the numbe of data professionals. Any empty values are dropped from job title and \n",
    "    salary so we will always have data. Other columns may have nans.\n",
    "    Data Manipulation:\n",
    "    - dropping nans from salary and devtype combined\n",
    "    - Changing the salary column to ConvertedCompYearly so we can merge all data frames comes time\n",
    "    - Converting specific columns that mean the same thing per year into a singular name\n",
    "    \"\"\"\n",
    "    frames = {}\n",
    "    stack_o_files = os.listdir(\"data/stack_overflow/\")\n",
    "    for file in stack_o_files:\n",
    "        year = file[-8:-4]\n",
    "        df = pd.read_csv(f\"data/stack_overflow/{file}\", encoding='ISO-8859-1')\n",
    "\n",
    "        # standardize compensation columns\n",
    "        if 'ConvertedComp' in df.columns:\n",
    "            df = df.rename(columns={'ConvertedComp': 'ConvertedCompYearly'})\n",
    "\n",
    "        # standardize some columns\n",
    "        standard = [\"Language\", \"Database\", \"Platform\", \"WebFrame\", \"MiscTech\"]\n",
    "        for stan in standard:\n",
    "            if f\"{stan}WorkedWith\" in df.columns:\n",
    "                df = df.rename(columns={f'{stan}WorkedWith': f'{stan}HaveWorkedWith', f'{stan}DesireNextYear':f'{stan}WantToWorkWith'})\n",
    "\n",
    "        # standardize some country names, now they should match with Kaggle dataset\n",
    "        df[\"Country\"] = df[\"Country\"].replace(country_abbreviations_1)\n",
    "        df[\"Country\"] = df[\"Country\"].replace(country_abbreviations_2)\n",
    "        \n",
    "        \n",
    "        df = df.dropna(subset=[\"DevType\", \"ConvertedCompYearly\"])\n",
    "        df = df[df[\"DevType\"].str.contains(\"data\", case=False)]\n",
    "        df[\"Year\"] = [year] * len(df)\n",
    "        frames[f\"df_data_{year}\"] = df\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a00a0790-540f-4d87-8677-9a8c762defa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/survey_results_public_2019.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m frames_dict \u001b[38;5;241m=\u001b[39m read_stackoverflow_csvs()\n",
      "Cell \u001b[1;32mIn[7], line 14\u001b[0m, in \u001b[0;36mread_stackoverflow_csvs\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m stack_o_files:\n\u001b[0;32m     13\u001b[0m     year \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m8\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m---> 14\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISO-8859-1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# standardize compensation columns\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConvertedComp\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\milestone_1\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\milestone_1\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\milestone_1\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\milestone_1\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\milestone_1\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/survey_results_public_2019.csv'"
     ]
    }
   ],
   "source": [
    "frames_dict = read_stackoverflow_csvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44403370-fb7b-4679-ac9b-856cf16b4db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the number of entries we are working with in our frames\n",
    "# seeing how to standardize the columns some more\n",
    "\n",
    "query = \"Remote\"\n",
    "for key, frame in frames_dict.items():\n",
    "    lang = []\n",
    "    for col in frame.columns:\n",
    "        lang.append(col) if query in col else None\n",
    "    print(f\"{key}\\t{len(frame)}\\t{lang}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b6973f-16be-4d0e-a9a2-543078294b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(frames_dict[\"df_data_2019\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e54b52-86b0-4468-a526-33b7dabc5db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(frames_dict[\"df_data_2020\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5382c8f8-589e-4817-8762-56ae2368768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(frames_dict[\"df_data_2021\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e954c060-4e0c-47b6-b32f-fd206ad8ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(frames_dict[\"df_data_2022\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47802c31-8420-41b5-8c3b-870494d64683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(frames_dict[\"df_data_2023\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "273a278b-e4c9-4ee9-8ad2-588f9757d841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do they have similar columns?\n",
    "def find_similar_col(frames) -> set:\n",
    "    \"\"\"\n",
    "    Returns the set of columns that the all share, ideally we maximize the ratio of this to merge.\n",
    "    \"\"\"\n",
    "    union = []\n",
    "    for key, frame in frames.items():\n",
    "        union.append(set(frame.columns))\n",
    "        \n",
    "    standard = union[0]\n",
    "    for cols in union[1:]:\n",
    "        standard = standard.intersection(cols)\n",
    "    return standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73d86b23-42fa-47a5-95cb-e1c00f86f9b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'frames_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m find_similar_col(frames_dict)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'frames_dict' is not defined"
     ]
    }
   ],
   "source": [
    "find_similar_col(frames_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "add4430f-be36-47eb-aed9-ff0b9a619e9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'frames_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# play around with the number and see if this is the spread that we want\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, frame \u001b[38;5;129;01min\u001b[39;00m frames_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      3\u001b[0m     grouped \u001b[38;5;241m=\u001b[39m (frame\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCountry\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcount())\n\u001b[0;32m      4\u001b[0m     grouped \u001b[38;5;241m=\u001b[39m grouped[grouped[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMainBranch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'frames_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# play around with the number and see if this is the spread that we want\n",
    "for key, frame in frames_dict.items():\n",
    "    grouped = (frame.groupby(\"Country\").count())\n",
    "    grouped = grouped[grouped[\"MainBranch\"] > 10]\n",
    "    length = len(grouped)\n",
    "    print(f\"\"\"{key}: {length}\n",
    "    max: {grouped['MainBranch'].idxmax()}, {grouped['MainBranch'].max()}\n",
    "    min: {grouped['MainBranch'].idxmin()}, {grouped['MainBranch'].min()}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c637099f-3869-4d84-97c5-89ddcc9bd3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do they have similar columns?\n",
    "def find_similar_country(frames: dict, cull_factor=20) -> set:\n",
    "    \"\"\"\n",
    "    Given a particular minimum (cull_factor) find the countries in common among\n",
    "    frames.\n",
    "    \"\"\"\n",
    "    union = []\n",
    "    for key, frame in frames.items():\n",
    "        grouped = frame.groupby(\"Country\").count()\n",
    "        grouped = grouped[grouped[\"MainBranch\"] > cull_factor]\n",
    "        union.append(set(grouped.index))\n",
    "        \n",
    "    standard = union[0]\n",
    "    for cols in union[1:]:\n",
    "        standard = standard.intersection(cols)\n",
    "    return standard\n",
    "\n",
    "def show_country_dist(frames: dict, countries: list, cull_factor: int) -> None:\n",
    "    \"\"\"\n",
    "    Just plot a bar chart for our country distributions using the above function.\n",
    "    \"\"\"\n",
    "    rows = len(frames)//2 + 1\n",
    "    fig, axes = plt.subplots(nrows=rows, ncols=2, figsize=(15,15))\n",
    "    fig.suptitle(f\"{len(countries)} respondents consistent across surveys greater than {cull_factor} responses\")\n",
    "    for (key, frame), ax in zip(frames.items(), axes.reshape(-1)):\n",
    "        grouped = frame.groupby(\"Country\").count()\n",
    "        grouped = grouped.loc[list(countries)].sort_values(\"MainBranch\")\n",
    "        grouped.plot(y=\"MainBranch\", ax=ax, kind=\"bar\", legend=False)\n",
    "        ax.set_title(key[-4:])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1e1d0c-5b5d-4f78-a0aa-460668f729f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# across all data sets here are the countries that are here most often\n",
    "# where is US? UK? They have different, inconsistent names throughout the years\n",
    "# # i.e. United States vs United States of America; UK vs United Kingdom, see above mapping\n",
    "cull_factor = 20\n",
    "country_sim = find_similar_country(frames_dict, cull_factor)\n",
    "show_country_dist(frames_dict, list(country_sim), cull_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8bc2c8-0f68-4509-aa9e-6b181b835b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b308f2b-2eeb-4ac0-aeaf-6711431064f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93304804-3ba2-4744-b32a-9c0663c443c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
