{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f55912e4-edcc-4792-b84f-f9a39975ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pycountry\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer\n",
    "pd.set_option('display.max_columns', 500)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "059736ef-2164-4596-98e8-f365110949d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luism\\anaconda3\\envs\\milestone_1\\Lib\\site-packages\\pycountry\\db.py:51: UserWarning: Country's official_name not found. Country name provided instead.\n",
      "  warnings.warn(warning_message, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ppp.csv', 'salaries.csv', 'stack_overflow']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_overflow_files = (os.listdir(\"data/\"))\n",
    "# not worth going from 2011-2014. No data scientists.\n",
    "# ok, so decision to do (2019 maybe) 2020-2023 for analysis\n",
    "\n",
    "# GPT gave me this idea instead of going through every possible country manually\n",
    "country_abbreviations_1 = {country.name: country.alpha_3 for country in pycountry.countries}\n",
    "country_abbreviations_2 = {country.official_name: country.alpha_3 for country in pycountry.countries}\n",
    "# os.listdir(\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27358111-7d4b-4047-b9f7-8b3026680695",
   "metadata": {},
   "source": [
    "Probably put the doc strings into markdowns\n",
    "- will also be used to explain visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f698f91-e30d-4e25-83b0-45fd3bb5176d",
   "metadata": {},
   "source": [
    "We could\n",
    "- merge on the money after rounding\n",
    "  - check how much data we have afterwards\n",
    "- like worldwide -> skills\n",
    "- function to merge and check the distribution afterwards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c841912-a2ba-4055-84b6-44b79d4064a5",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc27672e-a483-4135-bec9-523576a512ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_onehot_skills(frames: dict) -> None:\n",
    "    \"\"\"\n",
    "    Given a dictionary of pandas dataframes we want to one hot the skills in particular.\n",
    "    We want to take the skills in the different columns and one hot them such we can sum them for groupby operations.\n",
    "    We get a dictionary of pandas DataFrames and perform an inplace operation such that we don't have to create new memory.\n",
    "    Return a dictionary of a list of strings for a couple reasons:\n",
    "        - there's no way we will remember all of these so automation by putting these into a list seemed like the best idea\n",
    "        - the keys will match those in the input in case we want to do something with these later per year\n",
    "        - hashing onto a dictionary should allow for ease of access since no 2 years will have the same EXACT one hot columns, hence the list\n",
    "    The above is deprecated, after merging with similar columns these will all be useless to us\n",
    "\n",
    "    We also drop the _Empty for EVERYTHING since that information is useless to us\n",
    "    \n",
    "    Input: frames dict{str: pd.DataFrames}\n",
    "    Ouput: None\n",
    "\n",
    "    https://stackoverflow.com/questions/45312377/how-to-one-hot-encode-from-a-pandas-column-containing-a-list\n",
    "\n",
    "    Rough example flow of function for one sample:\n",
    "    C; C++; Perl -> [C, C++, Perl] -> [1, 1, 1, 0]\n",
    "    Python       -> [Python]       -> [0, 0, 0, 1]\n",
    "    \"\"\"\n",
    "    # some constants\n",
    "    standard = [(\"language\", \"lg\"), (\"database\", \"db\"), (\"platform\", \"pf\"), (\"webframe\", \"wf\"), (\"misctech\", \"mt\")]\n",
    "    status = [(\"wanttoworkwith\", \"www\"), (\"haveworkedwith\", \"hww\")]\n",
    "    \n",
    "    for key, frame in frames.items():\n",
    "        new_cols = []\n",
    "        for stan, abv in standard:\n",
    "            for stat, abr in status:\n",
    "                coi = stan + stat # coi = column of interest\n",
    "                abbr = abv + abr + \"_\"\n",
    "                mlb = MultiLabelBinarizer(sparse_output=True) # saves ram\n",
    "                frame[coi] = frame[coi].str.split(\";\")\n",
    "                transformed = mlb.fit_transform(frame.pop(coi))\n",
    "                new_cois = [abbr + name for name in mlb.classes_]\n",
    "                frame = frame.join(\n",
    "                            pd.DataFrame.sparse.from_spmatrix(\n",
    "                                transformed,\n",
    "                                index=frame.index,\n",
    "                                columns=new_cois\n",
    "                            )\n",
    "                        )\n",
    "                new_cois.remove(abbr + \"Empty\")\n",
    "                new_cols += new_cois\n",
    "                frame = frame.drop(abbr + \"Empty\", axis=1)\n",
    "        # this needs to be here, if not throse Sparse type errors\n",
    "        # # Sparse types don't allow normal groupby operations (ie reshape) so we need to turn them into ints\n",
    "        # # int8 don't take up a ton and it's just 0's and 1's\n",
    "        # # for all intents and purposes these are sparse matrices, we just want to avoid the object\n",
    "        frame[new_cols] = frame[new_cols].fillna(0)\n",
    "        frame[new_cols] = frame[new_cols].astype('int8')\n",
    "        frames[key] = frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c497e6d-c221-4d15-92c1-d1af1334b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbr_education(frames: dict) -> None:\n",
    "    \"\"\"\n",
    "    Similar in spirit to the other one hots, but this is in place\n",
    "    Automatically abbreviates education levels across all frames\n",
    "    Had to hard code the list again, not a big deal only 8 items\n",
    "    \n",
    "    Input: frames dict{str: pd.DataFrames}\n",
    "    Ouput: None\n",
    "    \"\"\"\n",
    "    # more hardcoded stuff that are needed\n",
    "    abbreviations = [\"Associate's\", \"Bachelor's\", \"Master's\", \"Elementary\", \"Professional\", \"Secondary\", \"Some College\", \"Else\"]\n",
    "    \n",
    "    for key, frame in frames.items():\n",
    "        # easier to replace this, makes it much easier to work with\n",
    "        frame['edlevel'] = frame['edlevel'].replace({'I never completed any formal education': 'Something else'})\n",
    "\n",
    "        # need the sorted since they have the same rough scheme\n",
    "        levels = list(frame['edlevel'].unique())\n",
    "        levels.sort()\n",
    "        o = 0 # offset\n",
    "\n",
    "        # dictionary to feed into repalce function\n",
    "        replace_dict = {}\n",
    "        for i in range(len(levels)):\n",
    "            col = levels[i]\n",
    "            if col == 'nan':\n",
    "                break\n",
    "            abbr = abbreviations[i-o]\n",
    "            if 'doctoral' in col:\n",
    "                replace_dict[col] = \"Doctoral\"\n",
    "                o += 1\n",
    "                continue\n",
    "            replace_dict[col] = abbr\n",
    "                \n",
    "        frame['edlevel'] = frame['edlevel'].replace(replace_dict)\n",
    "        frames[key] = frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09ca8e5b-5834-4349-a6df-ce3f218ddeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_ages(frames: dict) -> None:\n",
    "    \"\"\"\n",
    "    Bin ages so that they match with the later year surveys\n",
    "    \"\"\"\n",
    "    bins = [0, 18, 24, 34, 44, 54, 64, 100]\n",
    "    labels = ['Under 18 years old', '18-24 years old', '25-34 years old', '35-44 years old', '45-54 years old', '55-64 years old', '65 years or older']\n",
    "    for year, frame in frames.items():    \n",
    "        if frame[\"age\"].dtypes == float:\n",
    "            frame[\"age\"] = pd.cut(frame[\"age\"], bins=bins, labels=labels)\n",
    "        frame[\"age\"] = frame[\"age\"].astype('str')\n",
    "        \n",
    "        frames[year] = frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c4c5f8a-2213-4d15-9f74-bd17e707009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_col(frames) -> list:\n",
    "    \"\"\"\n",
    "    Returns the set of columns that the all share, ideally we maximize the ratio of this to merge.\n",
    "    \"\"\"\n",
    "    union = []\n",
    "    for key, frame in frames.items():\n",
    "        union.append(set(frame.columns))\n",
    "        \n",
    "    standard = union[0]\n",
    "    for cols in union[1:]:\n",
    "        standard = standard.intersection(cols)\n",
    "    return list(standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b4be010-96a0-4ee6-8509-d4f2d7cc50ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_devtype(df: pd.DataFrame) -> (pd.DataFrame, list):\n",
    "    \"\"\"\n",
    "    Standardizing DevType so that we can merge on ai-net data\n",
    "    \"\"\"\n",
    "    def map_job(category_list) -> list:\n",
    "        devtype = set()\n",
    "        for category in category_list:\n",
    "            if (clean := \"data scientist\") in category.lower():\n",
    "                devtype.add(clean)\n",
    "            elif \"math\" in category.lower() or \"stat\" in category.lower():\n",
    "                devtype.add(\"mathematician_statistician\")\n",
    "            elif (clean := \"analyst\") in category.lower():\n",
    "                devtype.add(clean)\n",
    "            elif (clean := \"manage\") in category.lower():\n",
    "                devtype.add(clean + \"ment\")\n",
    "            elif (clean := \"scientist\") in category.lower():\n",
    "                devtype.add(clean + \"_other\")\n",
    "            elif (clean := \"engineer\") in category.lower():\n",
    "                devtype.add(clean + \"_other\")\n",
    "            elif (clean := \"developer\") in category.lower():\n",
    "                devtype.add(clean)\n",
    "            else:\n",
    "                devtype.add(\"systems_architect\")\n",
    "        return list(devtype)\n",
    "        \n",
    "    coi = \"devtype\"\n",
    "    mlb = MultiLabelBinarizer(sparse_output=True) # saves ram\n",
    "    df[coi] = df[coi].str.split(\";\")\n",
    "    df[coi] = df[coi].apply(map_job)\n",
    "    transformed = mlb.fit_transform(df.pop(coi))\n",
    "    new_cols = mlb.classes_\n",
    "    df = df.join(\n",
    "                pd.DataFrame.sparse.from_spmatrix(\n",
    "                    transformed,\n",
    "                    index=df.index,\n",
    "                    columns=mlb.classes_\n",
    "                )\n",
    "            )\n",
    "    # see above binarizer\n",
    "    df[new_cols] = df[new_cols].fillna(0)\n",
    "    df[new_cols] = df[new_cols].astype('int8')\n",
    "    return df, new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10e6e980-8b30-4feb-872e-64cceaa62a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stackoverflow() -> (pd.DataFrame, list, list):\n",
    "    \"\"\"\n",
    "    Reads CSVs and gets the numbe of data professionals. Any empty values are dropped from job title and \n",
    "    salary so we will always have data. Other columns may have nans.\n",
    "    Data Manipulation:\n",
    "    - dropping nans from salary and devtype combined\n",
    "    - Changing the salary column to ConvertedCompYearly so we can merge all data frames comes time\n",
    "    - Lowering column names since there was some weird camel case going on\n",
    "    - Converting specific columns that mean the same thing per year into a singular name\n",
    "    - Fill in nans for language/skill specific values with \"nan\"\n",
    "      - this is so we can one hot later on for a more concise analysis, more later on\n",
    "    - Binarize the different skills per year, see create_onehot_skills\n",
    "    - Next we abbreviate education levels so that we can also one hot them, see above\n",
    "    - Change education to keep into one column binarizing doesn't make any sense\n",
    "    - Changing org size into something much more manageable, mainly the I don't know field\n",
    "    - We merge them into one, the same groupby operations can still be done as if seperate\n",
    "    - Encode devtype to binarize as well since it's very difficult to parse through ; every single time\n",
    "        - we can do some clever work arounds\n",
    "    - Lastly we return the skills columns really quick to save headache later on\n",
    "\n",
    "    Inputs: Nothing\n",
    "    Outputs: tuple(pd.DataFrame, list[str], list[str])\n",
    "    \"\"\"\n",
    "    frames = {}\n",
    "    stack_o_files = os.listdir(\"data/stack_overflow/\")\n",
    "    for file in stack_o_files:\n",
    "        year = file[-8:-4]\n",
    "        df = pd.read_csv(f\"data/stack_overflow/{file}\", encoding='ISO-8859-1')\n",
    "\n",
    "        # standardize compensation columns\n",
    "        if 'ConvertedComp' in df.columns:\n",
    "            df = df.rename(columns={'ConvertedComp': 'ConvertedCompYearly'})\n",
    "\n",
    "        # standardize some columns\n",
    "        # using camel case resulted in errors with webframe where sometimes F was capitalized\n",
    "        standard = [\"language\", \"database\", \"platform\", \"webframe\", \"misctech\"]\n",
    "        df.columns = df.columns.str.lower()\n",
    "        for stan in standard:\n",
    "            if f\"{stan}workedwith\" in df.columns:\n",
    "                df = df.rename(columns={f'{stan}workedwith': f'{stan}haveworkedwith', f'{stan}desirenextyear':f'{stan}wanttoworkwith'})\n",
    "            df[f\"{stan}haveworkedwith\"] = df[f\"{stan}haveworkedwith\"].fillna(value=\"Empty\")\n",
    "            df[f\"{stan}wanttoworkwith\"] = df[f\"{stan}wanttoworkwith\"].fillna(value=\"Empty\")\n",
    "\n",
    "        # standardize some country names, now they should match with Kaggle dataset\n",
    "        df[\"country\"] = df[\"country\"].replace(country_abbreviations_1)\n",
    "        df[\"country\"] = df[\"country\"].replace(country_abbreviations_2)\n",
    "\n",
    "        # we have some numbers so we can't just do entire df\n",
    "        df[['edlevel', 'orgsize']] = df[['edlevel', 'orgsize']].fillna(value=\"nan\")\n",
    "        df['orgsize'] = df['orgsize'].replace({'I donâ\\x80\\x99t know': 'IDK'})\n",
    "        \n",
    "        df = df.dropna(subset=[\"devtype\", \"convertedcompyearly\"])\n",
    "        df = df[df[\"devtype\"].str.contains(\"data\", case=False)]\n",
    "        df[\"count\"] = [1] * len(df) # this is for our groupby so that we can say count > cull when we sum or count\n",
    "        df[\"year\"] = [year] * len(df)\n",
    "        frames[f\"df_data_{year}\"] = df\n",
    "\n",
    "    # oops forgot indentation\n",
    "    abbr_education(frames)\n",
    "    bin_ages(frames)\n",
    "    create_onehot_skills(frames)\n",
    "    similar = find_similar_col(frames)\n",
    "    \n",
    "    # finally going to standardize to merge devtypes\n",
    "    for key, frame in frames.items():\n",
    "        frames[key] = frame[similar]\n",
    "    df = pd.concat([frame for key, frame in frames.items()], axis=0)\n",
    "    df, employment = encode_devtype(df)\n",
    "    skills = [col for col in df.columns if any(substr in col for substr in ['lg', 'db', 'pf', 'wf', 'mt'])]\n",
    "    df = df.reset_index().drop(\"index\")\n",
    "    return df, skills, employment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a8097b-2f9c-4802-952c-0bfeecd6fdfe",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79449a31-8bd9-4b64-b93f-0deedb21f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, skills, job_titles = read_stackoverflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dbf451e-9306-4f6e-9a0a-6f59a3e4e3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['analyst', 'data scientist', 'developer', 'engineer_other',\n",
       "       'management', 'scientist_other', 'systems_architect'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3d6c2fd-457f-4899-a4cc-e50fcc3b8c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['20 to 99 employees', 'nan', '10 to 19 employees',\n",
       "       '10,000 or more employees', '2 to 9 employees',\n",
       "       'Just me - I am a freelancer, sole proprietor, etc.',\n",
       "       '1,000 to 4,999 employees', '500 to 999 employees',\n",
       "       '5,000 to 9,999 employees', '100 to 499 employees',\n",
       "       '2-9 employees', 'IDK'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"orgsize\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee23bc3-c2b2-4501-9380-8d23af324b80",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Similarity with columns per the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "273a278b-e4c9-4ee9-8ad2-588f9757d841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do they have similar columns?\n",
    "def find_similar_col(frames) -> set:\n",
    "    \"\"\"\n",
    "    Returns the set of columns that the all share, ideally we maximize the ratio of this to merge.\n",
    "    \"\"\"\n",
    "    union = []\n",
    "    for key, frame in frames.items():\n",
    "        union.append(set(frame.columns))\n",
    "        \n",
    "    standard = union[0]\n",
    "    for cols in union[1:]:\n",
    "        standard = standard.intersection(cols)\n",
    "    return standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73d86b23-42fa-47a5-95cb-e1c00f86f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_similar_col(frames_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03c102a8-3dc3-4bb9-b9f9-311b1c46fd89",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'devtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\milestone_1\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'devtype'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_copy[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_copy[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevtype\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(map_job)\n\u001b[0;32m      2\u001b[0m df_copy[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\milestone_1\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\milestone_1\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'devtype'"
     ]
    }
   ],
   "source": [
    "# df_copy[\"test\"] = df_copy[\"devtype\"].apply(map_job)\n",
    "# df_copy[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac888058-a33c-4eb9-99b7-289d27e358ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy[[\"devtype\", \"test\"]].loc[88818].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3af8dde-b600-4800-88e7-58c062384788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3e2645-7fcc-4fa4-a61f-3fc1db7edb83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e8becc9-1c9c-4db7-ad19-aa5db3614321",
   "metadata": {},
   "source": [
    "## Countries given a cull factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add4430f-be36-47eb-aed9-ff0b9a619e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# play around with the number and see if this is the spread that we want\n",
    "for key, frame in frames_dict.items():\n",
    "    print(key)\n",
    "    grouped = frame.groupby(\"country\").count()\n",
    "    grouped = grouped[grouped[\"mainbranch\"] > 10]\n",
    "    length = len(grouped)\n",
    "    print(f\"\"\"{key}: {length}\n",
    "    max: {grouped['mainbranch'].idxmax()}, {grouped['mainbranch'].max()}\n",
    "    min: {grouped['mainbranch'].idxmin()}, {grouped['mainbranch'].min()}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c637099f-3869-4d84-97c5-89ddcc9bd3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do they have similar columns?\n",
    "def find_similar_country(frames: dict, cull_factor=20) -> set:\n",
    "    \"\"\"\n",
    "    Given a particular minimum (cull_factor) find the countries in common among\n",
    "    frames.\n",
    "    \"\"\"\n",
    "    union = []\n",
    "    for key, frame in frames.items():\n",
    "        grouped = frame.groupby(\"country\").count()\n",
    "        grouped = grouped[grouped[\"mainbranch\"] > cull_factor]\n",
    "        union.append(set(grouped.index))\n",
    "        \n",
    "    standard = union[0]\n",
    "    for cols in union[1:]:\n",
    "        standard = standard.intersection(cols)\n",
    "    return standard\n",
    "\n",
    "def show_country_dist(frames: dict, countries: list, cull_factor: int) -> None:\n",
    "    \"\"\"\n",
    "    Just plot a bar chart for our country distributions using the above function.\n",
    "    \"\"\"\n",
    "    rows = len(frames)//2 + 1\n",
    "    fig, axes = plt.subplots(nrows=rows, ncols=2, figsize=(15,15))\n",
    "    fig.suptitle(f\"{len(countries)} respondents consistent across surveys greater than {cull_factor} responses\")\n",
    "    for (key, frame), ax in zip(frames.items(), axes.reshape(-1)):\n",
    "        grouped = frame.groupby(\"country\").count()\n",
    "        grouped = grouped.loc[list(countries)].sort_values(\"mainbranch\")\n",
    "        grouped.plot(y=\"mainbranch\", ax=ax, kind=\"bar\", legend=False)\n",
    "        ax.set_title(key[-4:])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1e1d0c-5b5d-4f78-a0aa-460668f729f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# across all data sets here are the countries that are here most often\n",
    "# where is US? UK? They have different, inconsistent names throughout the years\n",
    "# # i.e. United States vs United States of America; UK vs United Kingdom, see above mapping\n",
    "cull_factor = 20\n",
    "country_sim = find_similar_country(frames_dict, cull_factor)\n",
    "# show_country_dist(frames_dict, list(country_sim), cull_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005703d4-1baf-4f62-80a1-7d0fb67882bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## One Hot Testing for Skills (deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b308f2b-2eeb-4ac0-aeaf-6711431064f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basically with every one of these is separated by a ;\n",
    "# goal of next function:\n",
    "# # find the sub-strings separated by ; nans will have to be replaced by \"None\" or \"Empty\"\n",
    "# # one hot the entries for example, if C appears in one of these queries, for that particular\n",
    "# # subject there will be a 1 for yes and 0 for no essentially\n",
    "# # this is why we need the None/Empty so we can add them up\n",
    "# # Eventually after one hotting we drop the None/Empty since it's a dummy column\n",
    "# # we would then be able to add them up using count or something and put onto a graph/analysis\n",
    "\n",
    "\n",
    "# standard = [\"language\", \"database\", \"platform\", \"webframe\", \"misctech\"]\n",
    "# want = \"wanttoworkwith\"\n",
    "# have = \"haveworkedwith\"\n",
    "# for key, frame in frames_dict.items():\n",
    "#     print(key)\n",
    "#     for stan in standard:\n",
    "#         print(f\"{stan}: {frame[stan + want].isna().sum()} {frame[stan + want].sample(n=1).values}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc1150b-2867-4d8f-aaa6-552cd78e92ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = frames_dict[\"df_data_2019\"].copy(deep=True) # don't want this to point at the frame in dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5204937d-79cd-43c9-9638-1f86471d7843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coi = 'languagewanttoworkwith'\n",
    "# df[coi] = df[coi].str.split(\";\")\n",
    "# mlb = MultiLabelBinarizer(sparse_output=True) # saves ram\n",
    "\n",
    "# transformed = mlb.fit_transform(df.pop(coi))\n",
    "# columns = [\"langwork_\" + name for name in mlb.classes_]\n",
    "\n",
    "# df = df.join(\n",
    "#             pd.DataFrame.sparse.from_spmatrix(\n",
    "#                 transformed,\n",
    "#                 index=df.index,\n",
    "#                 columns=columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b8f84-0576-4a88-9034-f836aa3b2034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame = frames_dict[\"df_data_2019\"].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a6427-f65e-49e5-a73e-57c91a53e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('country').sum()[mlb.classes_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef1474d-4069-4df1-b543-86c0d1cb275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_onehot_skills(frames: dict):\n",
    "#     # some constants\n",
    "#     standard = [(\"language\", \"lg\"), (\"database\", \"db\"), (\"platform\", \"pf\"), (\"webframe\", \"wf\"), (\"misctech\", \"mt\")]\n",
    "#     status = [(\"wanttoworkwith\", \"www\"), (\"haveworkedwith\", \"hww\")]\n",
    "\n",
    "#     new_cols_per_year = {}\n",
    "    \n",
    "#     for key, frame in frames.items():\n",
    "#         new_cols = []\n",
    "#         print(key)\n",
    "#         for stan, abv in standard:\n",
    "#             for stat, abr in status:\n",
    "#                 coi = stan + stat # coi = column of interest\n",
    "#                 abbr = abv + abr + \"_\"\n",
    "#                 mlb = MultiLabelBinarizer(sparse_output=True) # saves ram\n",
    "#                 frame[coi] = frame[coi].str.split(\";\")\n",
    "#                 transformed = mlb.fit_transform(frame.pop(coi))\n",
    "#                 new_cois = [abbr + name for name in mlb.classes_]\n",
    "#                 frame = frame.join(\n",
    "#                             pd.DataFrame.sparse.from_spmatrix(\n",
    "#                                 transformed,\n",
    "#                                 index=frame.index,\n",
    "#                                 columns=new_cois\n",
    "#                             )\n",
    "#                         )\n",
    "#                 new_cois.remove(abbr + \"Empty\")\n",
    "#                 new_cols += new_cois\n",
    "#                 frame.drop(abbr + \"Empty\", axis=1)\n",
    "#         frames[key] = frame\n",
    "#         new_cols_per_year[key] = new_cols\n",
    "#     return new_cols_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab559801-d6f4-463e-ab28-2080804e2f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# cp_dict = copy.deepcopy(frames_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d402790-a9e9-46fa-b52e-52fea6080c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_cols = create_onehot_skills(cp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36f2011-b9ce-4338-a00f-102d9bea271c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ed Level Processing (deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ab9aee-18d7-4fea-9306-949f9c8ce38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot education for same reason\n",
    "# same thing\n",
    "# we have nans and doctoral degrees missing from 2023 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c21f6c8-82d2-421e-a4fc-11f4640815a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# cp_dict = copy.deepcopy(frames_dict)\n",
    "# abbr_education(cp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f497be-d0d8-4eb8-9dce-b5167ddc381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, frame in cp_dict.items():\n",
    "#     frame['edlevel'] = frame['edlevel'].replace({'I never completed any formal education': 'Something else'})\n",
    "    \n",
    "#     do = list(frame['edlevel'].unique())\n",
    "#     print(key, len(do))\n",
    "#     do.sort()\n",
    "#     display(do)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc839d2-8939-447a-b7e7-1338b474e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, frame in cp_dict.items():\n",
    "#     lb = LabelBinarizer(sparse_output=True) # saves ram\n",
    "#     transformed = lb.fit_transform(frame.pop('edlevel'))\n",
    "#     frame = frame.join(\n",
    "#                 pd.DataFrame.sparse.from_spmatrix(\n",
    "#                     transformed,\n",
    "#                     index=frame.index,\n",
    "#                     columns=lb.classes_\n",
    "#                 )\n",
    "#             )\n",
    "#     if 'phd' not in frame.columns:\n",
    "#         frame['phd'] = [0] * len(frame)\n",
    "#     print(frame.columns[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd785f8-d94f-4bf3-9374-bf9f8398f887",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Employment (deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45a7b1b-822f-40d1-982e-9f97d9b3294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_similar_col(frames_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780df47f-f1af-42c5-8a0d-64ed0d71ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# col = \"devtype\"\n",
    "# for year in range(2019, 2024):\n",
    "#     frame = frames_dict[f\"df_data_{year}\"].copy()\n",
    "#     unique = frame[col].unique()\n",
    "#     # unique.sort()\n",
    "#     print(year, frame[col].dtypes, frame[col].isna().sum())\n",
    "#     print(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61180ed-ca75-4439-9bb3-007d15ec4451",
   "metadata": {},
   "source": [
    "## Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6963dc0a-3ee6-4f94-af9d-5e3a717b8dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = frames_dict[\"df_data_2019\"]\n",
    "grouped = df.groupby('country').agg({\"count\":[\"sum\"], \"convertedcompyearly\":[\"mean\", \"std\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e84c190-c6e0-445b-b6eb-80741dcd549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9696c46-295c-4aac-b128-62dd5448e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how we would cull, not awful but also not best thing in the world\n",
    "grouped = grouped[grouped[(\"count\", \"sum\")] > cull_factor]\n",
    "# grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785a3cf5-b9d0-4ef7-b2f4-99475925e647",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_dict[\"df_data_2020\"][\"devtype\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e076d-a15b-40c2-a845-361bd4b82280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
