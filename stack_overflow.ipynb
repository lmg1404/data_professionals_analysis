{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f55912e4-edcc-4792-b84f-f9a39975ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pycountry\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer\n",
    "pd.set_option('display.max_columns', 500)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "059736ef-2164-4596-98e8-f365110949d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ppp.csv', 'salaries.csv', 'stack_overflow']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_overflow_files = (os.listdir(\"data/\"))\n",
    "# not worth going from 2011-2014. No data scientists.\n",
    "# ok, so decision to do (2019 maybe) 2020-2023 for analysis\n",
    "\n",
    "# GPT gave me this idea instead of going through every possible country manually\n",
    "country_abbreviations_1 = {country.name: country.alpha_3 for country in pycountry.countries}\n",
    "country_abbreviations_2 = {country.official_name: country.alpha_3 for country in pycountry.countries}\n",
    "os.listdir(\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27358111-7d4b-4047-b9f7-8b3026680695",
   "metadata": {},
   "source": [
    "Probably put the doc strings into markdowns\n",
    "- will also be used to explain visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f698f91-e30d-4e25-83b0-45fd3bb5176d",
   "metadata": {},
   "source": [
    "We could\n",
    "- merge on the money after rounding\n",
    "  - check how much data we have afterwards\n",
    "- like worldwide -> skills\n",
    "- function to merge and check the distribution afterwards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c841912-a2ba-4055-84b6-44b79d4064a5",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fc27672e-a483-4135-bec9-523576a512ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_onehot_skills(frames: dict) -> None:\n",
    "    \"\"\"\n",
    "    Given a dictionary of pandas dataframes we want to one hot the skills in particular.\n",
    "    We want to take the skills in the different columns and one hot them such we can sum them for groupby operations.\n",
    "    We get a dictionary of pandas DataFrames and perform an inplace operation such that we don't have to create new memory.\n",
    "    Return a dictionary of a list of strings for a couple reasons:\n",
    "        - there's no way we will remember all of these so automation by putting these into a list seemed like the best idea\n",
    "        - the keys will match those in the input in case we want to do something with these later per year\n",
    "        - hashing onto a dictionary should allow for ease of access since no 2 years will have the same EXACT one hot columns, hence the list\n",
    "    The above is deprecated, after merging with similar columns these will all be useless to us\n",
    "\n",
    "    We also drop the _Empty for EVERYTHING since that information is useless to us\n",
    "    \n",
    "    Input: frames dict{str: pd.DataFrames}\n",
    "    Ouput: None\n",
    "\n",
    "    https://stackoverflow.com/questions/45312377/how-to-one-hot-encode-from-a-pandas-column-containing-a-list\n",
    "\n",
    "    Rough example flow of function for one sample:\n",
    "    C; C++; Perl -> [C, C++, Perl] -> [1, 1, 1, 0]\n",
    "    Python       -> [Python]       -> [0, 0, 0, 1]\n",
    "    \"\"\"\n",
    "    # some constants\n",
    "    standard = [(\"language\", \"lg\"), (\"database\", \"db\"), (\"platform\", \"pf\"), (\"webframe\", \"wf\"), (\"misctech\", \"mt\")]\n",
    "    status = [(\"wanttoworkwith\", \"www\"), (\"haveworkedwith\", \"hww\")]\n",
    "    \n",
    "    for key, frame in frames.items():\n",
    "        new_cols = []\n",
    "        for stan, abv in standard:\n",
    "            for stat, abr in status:\n",
    "                coi = stan + stat # coi = column of interest\n",
    "                abbr = abv + abr + \"_\"\n",
    "                mlb = MultiLabelBinarizer(sparse_output=True) # saves ram\n",
    "                frame[coi] = frame[coi].str.split(\";\")\n",
    "                transformed = mlb.fit_transform(frame.pop(coi))\n",
    "                new_cois = [abbr + name for name in mlb.classes_]\n",
    "                frame = frame.join(\n",
    "                            pd.DataFrame.sparse.from_spmatrix(\n",
    "                                transformed,\n",
    "                                index=frame.index,\n",
    "                                columns=new_cois\n",
    "                            )\n",
    "                        )\n",
    "                new_cois.remove(abbr + \"Empty\")\n",
    "                new_cols += new_cois\n",
    "                frame = frame.drop(abbr + \"Empty\", axis=1)\n",
    "        # this needs to be here, if not throse Sparse type errors\n",
    "        # # Sparse types don't allow normal groupby operations (ie reshape) so we need to turn them into ints\n",
    "        # # int8 don't take up a ton and it's just 0's and 1's\n",
    "        # # for all intents and purposes these are sparse matrices, we just want to avoid the object\n",
    "        frame[new_cols] = frame[new_cols].fillna(0)\n",
    "        frame[new_cols] = frame[new_cols].astype('int8')\n",
    "        frames[key] = frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0c497e6d-c221-4d15-92c1-d1af1334b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbr_education(frames: dict) -> None:\n",
    "    \"\"\"\n",
    "    Similar in spirit to the other one hots, but this is in place\n",
    "    Automatically abbreviates education levels across all frames\n",
    "    Had to hard code the list again, not a big deal only 8 items\n",
    "    \n",
    "    Input: frames dict{str: pd.DataFrames}\n",
    "    Ouput: None\n",
    "    \"\"\"\n",
    "    # more hardcoded stuff that are needed\n",
    "    abbreviations = [\"Associate's\", \"Bachelor's\", \"Master's\", \"Elementary\", \"Professional\", \"Secondary\", \"Some College\", \"Else\"]\n",
    "    \n",
    "    for key, frame in frames.items():\n",
    "        # easier to replace this, makes it much easier to work with\n",
    "        frame['edlevel'] = frame['edlevel'].replace({'I never completed any formal education': 'Something else'})\n",
    "\n",
    "        # need the sorted since they have the same rough scheme\n",
    "        levels = list(frame['edlevel'].unique())\n",
    "        levels.sort()\n",
    "        o = 0 # offset\n",
    "\n",
    "        # dictionary to feed into repalce function\n",
    "        replace_dict = {}\n",
    "        for i in range(len(levels)):\n",
    "            col = levels[i]\n",
    "            if col == 'nan':\n",
    "                break\n",
    "            abbr = abbreviations[i-o]\n",
    "            if 'doctoral' in col:\n",
    "                replace_dict[col] = \"Doctoral\"\n",
    "                o += 1\n",
    "                continue\n",
    "            replace_dict[col] = abbr\n",
    "                \n",
    "        frame['edlevel'] = frame['edlevel'].replace(replace_dict)\n",
    "        frames[key] = frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "09ca8e5b-5834-4349-a6df-ce3f218ddeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_ages(frames: dict) -> None:\n",
    "    bins = [0, 18, 24, 34, 44, 54, 64, 100]\n",
    "    labels = ['Under 18 years old', '18-24 years old', '25-34 years old', '35-44 years old', '45-54 years old', '55-64 years old', '65 years or older']\n",
    "    for year, frame in frames.items():    \n",
    "        if frame[\"age\"].dtypes == float:\n",
    "            frame[\"age\"] = pd.cut(frame[\"age\"], bins=bins, labels=labels)\n",
    "        frame[\"age\"] = frame[\"age\"].astype('str')\n",
    "        \n",
    "        frames[year] = frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9c4c5f8a-2213-4d15-9f74-bd17e707009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_col(frames) -> list:\n",
    "    \"\"\"\n",
    "    Returns the set of columns that the all share, ideally we maximize the ratio of this to merge.\n",
    "    \"\"\"\n",
    "    union = []\n",
    "    for key, frame in frames.items():\n",
    "        union.append(set(frame.columns))\n",
    "        \n",
    "    standard = union[0]\n",
    "    for cols in union[1:]:\n",
    "        standard = standard.intersection(cols)\n",
    "    return list(standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3b4be010-96a0-4ee6-8509-d4f2d7cc50ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_devtype(df: pd.DataFrame) -> (pd.DataFrame, list):\n",
    "    coi = \"devtype\"\n",
    "    mlb = MultiLabelBinarizer(sparse_output=True) # saves ram\n",
    "    df[coi] = df[coi].str.split(\";\")\n",
    "    transformed = mlb.fit_transform(df.pop(coi))\n",
    "    new_cols = mlb.classes_\n",
    "    df = df.join(\n",
    "                pd.DataFrame.sparse.from_spmatrix(\n",
    "                    transformed,\n",
    "                    index=df.index,\n",
    "                    columns=mlb.classes_\n",
    "                )\n",
    "            )\n",
    "    # see above binarizer\n",
    "    df[new_cols] = df[new_cols].fillna(0)\n",
    "    df[new_cols] = df[new_cols].astype('int8')\n",
    "    return df, new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "10e6e980-8b30-4feb-872e-64cceaa62a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stackoverflow() -> (pd.DataFrame, list, list):\n",
    "    \"\"\"\n",
    "    Reads CSVs and gets the numbe of data professionals. Any empty values are dropped from job title and \n",
    "    salary so we will always have data. Other columns may have nans.\n",
    "    Data Manipulation:\n",
    "    - dropping nans from salary and devtype combined\n",
    "    - Changing the salary column to ConvertedCompYearly so we can merge all data frames comes time\n",
    "    - Lowering column names since there was some weird camel case going on\n",
    "    - Converting specific columns that mean the same thing per year into a singular name\n",
    "    - Fill in nans for language/skill specific values with \"nan\"\n",
    "      - this is so we can one hot later on for a more concise analysis, more later on\n",
    "    - Binarize the different skills per year, see create_onehot_skills\n",
    "    - Next we abbreviate education levels so that we can also one hot them, see above\n",
    "    - Change education to keep into one column binarizing doesn't make any sense\n",
    "    - Changing org size into something much more manageable, mainly the I don't know field\n",
    "    - We merge them into one, the same groupby operations can still be done as if seperate\n",
    "    - Encode devtype to binarize as well since it's very difficult to parse through ; every single time\n",
    "        - we can do some clever work arounds\n",
    "    - Lastly we return the skills columns really quick to save headache later on\n",
    "\n",
    "    Inputs: Nothing\n",
    "    Outputs: tuple(pd.DataFrame, list[str], list[str])\n",
    "    \"\"\"\n",
    "    frames = {}\n",
    "    stack_o_files = os.listdir(\"data/stack_overflow/\")\n",
    "    for file in stack_o_files:\n",
    "        year = file[-8:-4]\n",
    "        df = pd.read_csv(f\"data/stack_overflow/{file}\", encoding='ISO-8859-1')\n",
    "\n",
    "        # standardize compensation columns\n",
    "        if 'ConvertedComp' in df.columns:\n",
    "            df = df.rename(columns={'ConvertedComp': 'ConvertedCompYearly'})\n",
    "\n",
    "        # standardize some columns\n",
    "        # using camel case resulted in errors with webframe where sometimes F was capitalized\n",
    "        standard = [\"language\", \"database\", \"platform\", \"webframe\", \"misctech\"]\n",
    "        df.columns = df.columns.str.lower()\n",
    "        for stan in standard:\n",
    "            if f\"{stan}workedwith\" in df.columns:\n",
    "                df = df.rename(columns={f'{stan}workedwith': f'{stan}haveworkedwith', f'{stan}desirenextyear':f'{stan}wanttoworkwith'})\n",
    "            df[f\"{stan}haveworkedwith\"] = df[f\"{stan}haveworkedwith\"].fillna(value=\"Empty\")\n",
    "            df[f\"{stan}wanttoworkwith\"] = df[f\"{stan}wanttoworkwith\"].fillna(value=\"Empty\")\n",
    "\n",
    "        # standardize some country names, now they should match with Kaggle dataset\n",
    "        df[\"country\"] = df[\"country\"].replace(country_abbreviations_1)\n",
    "        df[\"country\"] = df[\"country\"].replace(country_abbreviations_2)\n",
    "\n",
    "        # we have some numbers so we can't just do entire df\n",
    "        df[['edlevel', 'orgsize']] = df[['edlevel', 'orgsize']].fillna(value=\"nan\")\n",
    "        df['orgsize'] = df['orgsize'].replace({'I donâ\\x80\\x99t know': 'IDK'})\n",
    "        \n",
    "        df = df.dropna(subset=[\"devtype\", \"convertedcompyearly\"])\n",
    "        df = df[df[\"devtype\"].str.contains(\"data\", case=False)]\n",
    "        df[\"count\"] = [1] * len(df) # this is for our groupby so that we can say count > cull when we sum or count\n",
    "        df[\"year\"] = [year] * len(df)\n",
    "        frames[f\"df_data_{year}\"] = df\n",
    "\n",
    "    # oops forgot indentation\n",
    "    abbr_education(frames)\n",
    "    bin_ages(frames)\n",
    "    create_onehot_skills(frames)\n",
    "    similar = find_similar_col(frames)\n",
    "    \n",
    "    # finally going to standardize to merge devtypes\n",
    "    for key, frame in frames.items():\n",
    "        frames[key] = frame[similar]\n",
    "    df = pd.concat([frame for key, frame in frames.items()], axis=0)\n",
    "    df, employment = encode_devtype(df)\n",
    "    skills = [col for col in df.columns if any(substr in col for substr in ['lg', 'db', 'pf', 'wf', 'mt'])]\n",
    "    \n",
    "    return df, skills, employment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a8097b-2f9c-4802-952c-0bfeecd6fdfe",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "79449a31-8bd9-4b64-b93f-0deedb21f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, skills, job_titles = read_stackoverflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9286ad21-0efe-4434-ba47-7080f61b6372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "35d213b7-8f49-44c3-8789-2fc652b539ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = frames_dict[\"df_data_2019\"]\n",
    "# skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1755cb56-6785-4ade-8ef7-3fcf15945bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dtypes[df. dtypes == 'Sparse[int32, 0]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9a5074c0-27cd-45da-a180-c5c5ed5aad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(frames_dict[\"df_data_2019\"].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44403370-fb7b-4679-ac9b-856cf16b4db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_data_2019\t13393\t['lgwww_WebAssembly', 'lghww_WebAssembly']\n",
      "df_data_2020\t8294\t[]\n",
      "df_data_2021\t9272\t[]\n",
      "df_data_2022\t6921\t[]\n",
      "df_data_2023\t2480\t['pfwww_Amazon Web Services (AWS)', 'pfhww_Amazon Web Services (AWS)']\n"
     ]
    }
   ],
   "source": [
    "# this is the number of entries we are working with in our frames\n",
    "# seeing how to standardize the columns some more\n",
    "# this is kind of useless now with one hotting everything\n",
    "\n",
    "query = \"Web\"\n",
    "for key, frame in frames_dict.items():\n",
    "    lang = []\n",
    "    for col in frame.columns:\n",
    "        lang.append(col) if query in col else None\n",
    "    print(f\"{key}\\t{len(frame)}\\t{lang}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8b6973f-16be-4d0e-a9a2-543078294b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(frames_dict[\"df_data_2019\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89e54b52-86b0-4468-a526-33b7dabc5db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(frames_dict[\"df_data_2020\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5382c8f8-589e-4817-8762-56ae2368768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(frames_dict[\"df_data_2021\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e954c060-4e0c-47b6-b32f-fd206ad8ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(frames_dict[\"df_data_2022\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47802c31-8420-41b5-8c3b-870494d64683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(frames_dict[\"df_data_2023\"].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee23bc3-c2b2-4501-9380-8d23af324b80",
   "metadata": {},
   "source": [
    "## Similarity with columns per the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "273a278b-e4c9-4ee9-8ad2-588f9757d841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do they have similar columns?\n",
    "def find_similar_col(frames) -> set:\n",
    "    \"\"\"\n",
    "    Returns the set of columns that the all share, ideally we maximize the ratio of this to merge.\n",
    "    \"\"\"\n",
    "    union = []\n",
    "    for key, frame in frames.items():\n",
    "        union.append(set(frame.columns))\n",
    "        \n",
    "    standard = union[0]\n",
    "    for cols in union[1:]:\n",
    "        standard = standard.intersection(cols)\n",
    "    return standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "73d86b23-42fa-47a5-95cb-e1c00f86f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_similar_col(frames_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8becc9-1c9c-4db7-ad19-aa5db3614321",
   "metadata": {},
   "source": [
    "## Countries given a cull factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "add4430f-be36-47eb-aed9-ff0b9a619e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_data_2019\n",
      "df_data_2019: 83\n",
      "    max: USA, 3856\n",
      "    min: ARM, 11\n",
      "df_data_2020\n",
      "df_data_2020: 69\n",
      "    max: USA, 2081\n",
      "    min: BLR, 11\n",
      "df_data_2021\n",
      "df_data_2021: 70\n",
      "    max: USA, 2144\n",
      "    min: BIH, 11\n",
      "df_data_2022\n",
      "df_data_2022: 63\n",
      "    max: USA, 1702\n",
      "    min: EGY, 11\n",
      "df_data_2023\n",
      "df_data_2023: 36\n",
      "    max: USA, 687\n",
      "    min: CHN, 11\n"
     ]
    }
   ],
   "source": [
    "# play around with the number and see if this is the spread that we want\n",
    "for key, frame in frames_dict.items():\n",
    "    print(key)\n",
    "    grouped = frame.groupby(\"country\").count()\n",
    "    grouped = grouped[grouped[\"mainbranch\"] > 10]\n",
    "    length = len(grouped)\n",
    "    print(f\"\"\"{key}: {length}\n",
    "    max: {grouped['mainbranch'].idxmax()}, {grouped['mainbranch'].max()}\n",
    "    min: {grouped['mainbranch'].idxmin()}, {grouped['mainbranch'].min()}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c637099f-3869-4d84-97c5-89ddcc9bd3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do they have similar columns?\n",
    "def find_similar_country(frames: dict, cull_factor=20) -> set:\n",
    "    \"\"\"\n",
    "    Given a particular minimum (cull_factor) find the countries in common among\n",
    "    frames.\n",
    "    \"\"\"\n",
    "    union = []\n",
    "    for key, frame in frames.items():\n",
    "        grouped = frame.groupby(\"country\").count()\n",
    "        grouped = grouped[grouped[\"mainbranch\"] > cull_factor]\n",
    "        union.append(set(grouped.index))\n",
    "        \n",
    "    standard = union[0]\n",
    "    for cols in union[1:]:\n",
    "        standard = standard.intersection(cols)\n",
    "    return standard\n",
    "\n",
    "def show_country_dist(frames: dict, countries: list, cull_factor: int) -> None:\n",
    "    \"\"\"\n",
    "    Just plot a bar chart for our country distributions using the above function.\n",
    "    \"\"\"\n",
    "    rows = len(frames)//2 + 1\n",
    "    fig, axes = plt.subplots(nrows=rows, ncols=2, figsize=(15,15))\n",
    "    fig.suptitle(f\"{len(countries)} respondents consistent across surveys greater than {cull_factor} responses\")\n",
    "    for (key, frame), ax in zip(frames.items(), axes.reshape(-1)):\n",
    "        grouped = frame.groupby(\"country\").count()\n",
    "        grouped = grouped.loc[list(countries)].sort_values(\"mainbranch\")\n",
    "        grouped.plot(y=\"mainbranch\", ax=ax, kind=\"bar\", legend=False)\n",
    "        ax.set_title(key[-4:])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b1e1d0c-5b5d-4f78-a0aa-460668f729f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# across all data sets here are the countries that are here most often\n",
    "# where is US? UK? They have different, inconsistent names throughout the years\n",
    "# # i.e. United States vs United States of America; UK vs United Kingdom, see above mapping\n",
    "cull_factor = 20\n",
    "country_sim = find_similar_country(frames_dict, cull_factor)\n",
    "# show_country_dist(frames_dict, list(country_sim), cull_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005703d4-1baf-4f62-80a1-7d0fb67882bd",
   "metadata": {},
   "source": [
    "## One Hot Testing for Skills (deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b308f2b-2eeb-4ac0-aeaf-6711431064f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basically with every one of these is separated by a ;\n",
    "# goal of next function:\n",
    "# # find the sub-strings separated by ; nans will have to be replaced by \"None\" or \"Empty\"\n",
    "# # one hot the entries for example, if C appears in one of these queries, for that particular\n",
    "# # subject there will be a 1 for yes and 0 for no essentially\n",
    "# # this is why we need the None/Empty so we can add them up\n",
    "# # Eventually after one hotting we drop the None/Empty since it's a dummy column\n",
    "# # we would then be able to add them up using count or something and put onto a graph/analysis\n",
    "\n",
    "\n",
    "# standard = [\"language\", \"database\", \"platform\", \"webframe\", \"misctech\"]\n",
    "# want = \"wanttoworkwith\"\n",
    "# have = \"haveworkedwith\"\n",
    "# for key, frame in frames_dict.items():\n",
    "#     print(key)\n",
    "#     for stan in standard:\n",
    "#         print(f\"{stan}: {frame[stan + want].isna().sum()} {frame[stan + want].sample(n=1).values}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddc1150b-2867-4d8f-aaa6-552cd78e92ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = frames_dict[\"df_data_2019\"].copy(deep=True) # don't want this to point at the frame in dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5204937d-79cd-43c9-9638-1f86471d7843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coi = 'languagewanttoworkwith'\n",
    "# df[coi] = df[coi].str.split(\";\")\n",
    "# mlb = MultiLabelBinarizer(sparse_output=True) # saves ram\n",
    "\n",
    "# transformed = mlb.fit_transform(df.pop(coi))\n",
    "# columns = [\"langwork_\" + name for name in mlb.classes_]\n",
    "\n",
    "# df = df.join(\n",
    "#             pd.DataFrame.sparse.from_spmatrix(\n",
    "#                 transformed,\n",
    "#                 index=df.index,\n",
    "#                 columns=columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "008b8f84-0576-4a88-9034-f836aa3b2034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame = frames_dict[\"df_data_2019\"].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "652a6427-f65e-49e5-a73e-57c91a53e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('country').sum()[mlb.classes_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fef1474d-4069-4df1-b543-86c0d1cb275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_onehot_skills(frames: dict):\n",
    "#     # some constants\n",
    "#     standard = [(\"language\", \"lg\"), (\"database\", \"db\"), (\"platform\", \"pf\"), (\"webframe\", \"wf\"), (\"misctech\", \"mt\")]\n",
    "#     status = [(\"wanttoworkwith\", \"www\"), (\"haveworkedwith\", \"hww\")]\n",
    "\n",
    "#     new_cols_per_year = {}\n",
    "    \n",
    "#     for key, frame in frames.items():\n",
    "#         new_cols = []\n",
    "#         print(key)\n",
    "#         for stan, abv in standard:\n",
    "#             for stat, abr in status:\n",
    "#                 coi = stan + stat # coi = column of interest\n",
    "#                 abbr = abv + abr + \"_\"\n",
    "#                 mlb = MultiLabelBinarizer(sparse_output=True) # saves ram\n",
    "#                 frame[coi] = frame[coi].str.split(\";\")\n",
    "#                 transformed = mlb.fit_transform(frame.pop(coi))\n",
    "#                 new_cois = [abbr + name for name in mlb.classes_]\n",
    "#                 frame = frame.join(\n",
    "#                             pd.DataFrame.sparse.from_spmatrix(\n",
    "#                                 transformed,\n",
    "#                                 index=frame.index,\n",
    "#                                 columns=new_cois\n",
    "#                             )\n",
    "#                         )\n",
    "#                 new_cois.remove(abbr + \"Empty\")\n",
    "#                 new_cols += new_cois\n",
    "#                 frame.drop(abbr + \"Empty\", axis=1)\n",
    "#         frames[key] = frame\n",
    "#         new_cols_per_year[key] = new_cols\n",
    "#     return new_cols_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab559801-d6f4-463e-ab28-2080804e2f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# cp_dict = copy.deepcopy(frames_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d402790-a9e9-46fa-b52e-52fea6080c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_cols = create_onehot_skills(cp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36f2011-b9ce-4338-a00f-102d9bea271c",
   "metadata": {},
   "source": [
    "## Ed Level Processing (deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29ab9aee-18d7-4fea-9306-949f9c8ce38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot education for same reason\n",
    "# same thing\n",
    "# we have nans and doctoral degrees missing from 2023 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c21f6c8-82d2-421e-a4fc-11f4640815a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# cp_dict = copy.deepcopy(frames_dict)\n",
    "# abbr_education(cp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54f497be-d0d8-4eb8-9dce-b5167ddc381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, frame in cp_dict.items():\n",
    "#     frame['edlevel'] = frame['edlevel'].replace({'I never completed any formal education': 'Something else'})\n",
    "    \n",
    "#     do = list(frame['edlevel'].unique())\n",
    "#     print(key, len(do))\n",
    "#     do.sort()\n",
    "#     display(do)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0bc839d2-8939-447a-b7e7-1338b474e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, frame in cp_dict.items():\n",
    "#     lb = LabelBinarizer(sparse_output=True) # saves ram\n",
    "#     transformed = lb.fit_transform(frame.pop('edlevel'))\n",
    "#     frame = frame.join(\n",
    "#                 pd.DataFrame.sparse.from_spmatrix(\n",
    "#                     transformed,\n",
    "#                     index=frame.index,\n",
    "#                     columns=lb.classes_\n",
    "#                 )\n",
    "#             )\n",
    "#     if 'phd' not in frame.columns:\n",
    "#         frame['phd'] = [0] * len(frame)\n",
    "#     print(frame.columns[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd785f8-d94f-4bf3-9374-bf9f8398f887",
   "metadata": {},
   "source": [
    "## Employment (deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e45a7b1b-822f-40d1-982e-9f97d9b3294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_similar_col(frames_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "780df47f-f1af-42c5-8a0d-64ed0d71ab10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 object 0\n",
      "['Data or business analyst;Data scientist or machine learning specialist;Database administrator;Engineer, data'\n",
      " 'Database administrator;Developer, back-end;Developer, front-end;Developer, full-stack;Developer, QA or test;DevOps specialist'\n",
      " 'Data or business analyst;Data scientist or machine learning specialist;Database administrator;Developer, back-end;Developer, desktop or enterprise applications;Developer, front-end;Developer, full-stack;Developer, game or graphics;Educator'\n",
      " ...\n",
      " 'Data scientist or machine learning specialist;Engineer, data;Engineering manager;Product manager'\n",
      " 'Data scientist or machine learning specialist;Developer, desktop or enterprise applications;Developer, full-stack;DevOps specialist'\n",
      " 'Academic researcher;Database administrator;Developer, back-end;Developer, desktop or enterprise applications;Developer, embedded applications or devices;Developer, front-end;Developer, full-stack;Developer, game or graphics;Developer, mobile;Educator;Marketing or sales professional;Product manager;Scientist;System administrator']\n",
      "2020 object 0\n",
      "['Database administrator;Developer, full-stack;Developer, mobile'\n",
      " 'Database administrator;Developer, back-end;Developer, desktop or enterprise applications;Developer, full-stack;Developer, QA or test;DevOps specialist'\n",
      " 'Data or business analyst;Database administrator;Developer, back-end;Developer, front-end;Developer, full-stack;System administrator'\n",
      " ...\n",
      " 'Developer, back-end;Developer, front-end;Developer, full-stack;DevOps specialist;Engineer, data;Engineering manager'\n",
      " 'Data scientist or machine learning specialist;Designer;Developer, back-end;Developer, desktop or enterprise applications;Developer, front-end;Developer, full-stack;Developer, mobile;Engineer, data;Engineer, site reliability;Engineering manager'\n",
      " 'Database administrator;Developer, back-end;Developer, desktop or enterprise applications;Developer, front-end;Developer, full-stack;Engineer, data;Engineering manager;System administrator']\n",
      "2021 object 0\n",
      "['Data scientist or machine learning specialist'\n",
      " 'Engineer, data;Data scientist or machine learning specialist'\n",
      " 'Developer, desktop or enterprise applications;Developer, full-stack;Developer, back-end;Database administrator'\n",
      " ...\n",
      " 'Developer, front-end;Developer, desktop or enterprise applications;Developer, full-stack;Developer, back-end;Database administrator;Designer;Engineer, site reliability;System administrator'\n",
      " 'Data scientist or machine learning specialist;Developer, back-end;Academic researcher;Scientist;Student;Engineering manager'\n",
      " 'Developer, mobile;Developer, desktop or enterprise applications;Data scientist or machine learning specialist;Developer, back-end;Engineering manager']\n",
      "2022 object 0\n",
      "['Data scientist or machine learning specialist;Developer, front-end;Engineer, data;Engineer, site reliability'\n",
      " 'Engineer, data'\n",
      " 'Developer, front-end;Engineer, data;Developer, full-stack;Developer, back-end;Developer, desktop or enterprise applications;Developer, QA or test;Developer, mobile;Database administrator;Developer, embedded applications or devices;Cloud infrastructure engineer;Data or business analyst;Designer'\n",
      " ... 'Developer, mobile;Database administrator'\n",
      " 'Developer, front-end;Developer, full-stack;Developer, back-end;Database administrator;DevOps specialist;Project manager;System administrator'\n",
      " 'Developer, front-end;Developer, full-stack;Developer, back-end;Developer, desktop or enterprise applications;Database administrator;Developer, embedded applications or devices;Cloud infrastructure engineer']\n",
      "2023 object 0\n",
      "['Data scientist or machine learning specialist'\n",
      " 'Data or business analyst' 'Database administrator' 'Engineer, data']\n"
     ]
    }
   ],
   "source": [
    "col = \"devtype\"\n",
    "for year in range(2019, 2024):\n",
    "    frame = frames_dict[f\"df_data_{year}\"].copy()\n",
    "    unique = frame[col].unique()\n",
    "    # unique.sort()\n",
    "    print(year, frame[col].dtypes, frame[col].isna().sum())\n",
    "    print(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6963dc0a-3ee6-4f94-af9d-5e3a717b8dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = frames_dict[\"df_data_2019\"]\n",
    "grouped = df.groupby('country').agg({\"count\":[\"sum\"], \"convertedcompyearly\":[\"mean\", \"std\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e84c190-c6e0-445b-b6eb-80741dcd549a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiIndex([(              'count',  'sum'),\n",
       "            ('convertedcompyearly', 'mean'),\n",
       "            ('convertedcompyearly',  'std')],\n",
       "           )"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c9696c46-295c-4aac-b128-62dd5448e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how we would cull, not awful but also not best thing in the world\n",
    "grouped = grouped[grouped[(\"count\", \"sum\")] > cull_factor]\n",
    "# grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "785a3cf5-b9d0-4ef7-b2f4-99475925e647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "devtype\n",
       "Data scientist or machine learning specialist                                                                                                                                                  228\n",
       "Developer, back-end;Engineer, data                                                                                                                                                             184\n",
       "Data or business analyst                                                                                                                                                                       157\n",
       "Data or business analyst;Data scientist or machine learning specialist                                                                                                                         129\n",
       "Engineer, data                                                                                                                                                                                 118\n",
       "                                                                                                                                                                                              ... \n",
       "Data or business analyst;Developer, front-end;Developer, QA or test;Educator                                                                                                                     1\n",
       "Academic researcher;Data or business analyst;Data scientist or machine learning specialist;Database administrator;DevOps specialist;Engineer, data;Scientist                                     1\n",
       "Academic researcher;Data or business analyst;Developer, desktop or enterprise applications;Developer, QA or test;Educator;Product manager;System administrator                                   1\n",
       "Data or business analyst;Database administrator;Developer, back-end;Developer, desktop or enterprise applications;Developer, embedded applications or devices;Developer, full-stack              1\n",
       "Database administrator;Developer, back-end;Developer, desktop or enterprise applications;Developer, front-end;Developer, full-stack;Engineer, data;Engineering manager;System administrator      1\n",
       "Name: count, Length: 3728, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames_dict[\"df_data_2020\"][\"devtype\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e076d-a15b-40c2-a845-361bd4b82280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
